{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PAGERANK FOR AMAZON BOOK REVIEWS\n",
        "## AMD - University of Milan\n",
        "### Fatemeh Amirian 34015A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PHASE ZERO: SET UP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_d05AmVNc7z",
        "outputId": "75776aea-6dac-47ac-988b-be0c85d76a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.5.1)\n",
            "Requirement already satisfied: graphframes in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6)\n",
            "Requirement already satisfied: kaggle in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.7.4.5)\n",
            "Requirement already satisfied: findspark in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from graphframes) (2.1.2)\n",
            "Requirement already satisfied: nose in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from graphframes) (1.3.7)\n",
            "Requirement already satisfied: bleach in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (6.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from kaggle) (65.5.0)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\diana\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->kaggle) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark graphframes kaggle findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "# Third-party imports\n",
        "import findspark\n",
        "import matplotlib.pyplot as plt\n",
        "from graphframes import GraphFrame\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import (\n",
        "    DoubleType,\n",
        "    LongType,\n",
        "    StringType,\n",
        "    StructField,\n",
        "    StructType\n",
        ")\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import BooleanType\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "winutils.exe already exists!\n"
          ]
        }
      ],
      "source": [
        "# Create Hadoop directory and download winutils.exe\n",
        "import urllib.request\n",
        "\n",
        "# Create the hadoop/bin directory\n",
        "hadoop_dir = \"C:\\\\hadoop\\\\bin\"\n",
        "os.makedirs(hadoop_dir, exist_ok=True)\n",
        "\n",
        "# Download winutils.exe\n",
        "winutils_url = \"https://github.com/steveloughran/winutils/raw/master/hadoop-2.7.1/bin/winutils.exe\"\n",
        "winutils_path = os.path.join(hadoop_dir, \"winutils.exe\")\n",
        "\n",
        "if not os.path.exists(winutils_path):\n",
        "    print(\"Downloading winutils.exe...\")\n",
        "    urllib.request.urlretrieve(winutils_url, winutils_path)\n",
        "    print(\"Downloaded winutils.exe successfully!\")\n",
        "else:\n",
        "    print(\"winutils.exe already exists!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set Java path for Windows (adjust path based on your Java installation)\n",
        "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-8.0.452.9-hotspot\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-memory 8g pyspark-shell\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GraphFrames imported successfully!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    sc.stop()  \n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "# Initialize findspark first\n",
        "findspark.init()\n",
        "\n",
        "# Set Hadoop home properly for Windows\n",
        "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \";C:\\\\hadoop\\\\bin\"\n",
        "\n",
        "# Setup Spark with GraphFrames support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PageRankForAmazonBookReviews\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Test GraphFrames import\n",
        "from graphframes import GraphFrame\n",
        "print(\"GraphFrames imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "C_O6SwUOORlv",
        "outputId": "f2c63f4a-2db6-4788-e8b4-74e81581368a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://DianA.station:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PageRankForAmazonBookReviews</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x291ffa7fd10>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TBqHUCkOdyD"
      },
      "source": [
        "Downloading the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcX1qyZZOfvP",
        "outputId": "078da827-f63c-4154-8376-00d94ef227b6"
      },
      "outputs": [],
      "source": [
        "# Replace \"xxxxxx\" with your actual Kaggle username and API key\n",
        "os.environ['KAGGLE_USERNAME'] = \"xxxxxx\"\n",
        "os.environ['KAGGLE_KEY'] = \"xxxxxx\"\n",
        "\n",
        "# Download Amazon Books Review dataset\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N88NFEu5O9JP"
      },
      "outputs": [],
      "source": [
        "# prompt: unzip the data\n",
        "\n",
        "with zipfile.ZipFile('amazon-books-reviews.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PHASE ONE : DATA PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYOFhfIlPCM1",
        "outputId": "c627f116-cb1e-4def-d962-6e16a13328f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- review/score: double (nullable = true)\n",
            "\n",
            "+----------+------------------------------+--------------+------------+\n",
            "|Id        |Title                         |User_id       |review/score|\n",
            "+----------+------------------------------+--------------+------------+\n",
            "|1882931173|Its Only Art If Its Well Hung!|AVCGYZL8FQQTD |4.0         |\n",
            "|0826414346|Dr. Seuss: American Icon      |A30TK6U7DNS82R|5.0         |\n",
            "|0826414346|Dr. Seuss: American Icon      |A3UH4UZ4RSVO82|5.0         |\n",
            "|0826414346|Dr. Seuss: American Icon      |A2MVUWT453QH61|4.0         |\n",
            "|0826414346|Dr. Seuss: American Icon      |A22X4XUPKF66MR|4.0         |\n",
            "+----------+------------------------------+--------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# reading the dataset and selecting the required columns\n",
        "selected_book_reviews = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"quote\", '\"') \\\n",
        "    .option(\"escape\", '\"') \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .csv(\"D:\\AMD\\data\\Books_rating.csv\") \\\n",
        "    .select(\"Id\", \"Title\", \"User_id\", \"review/score\")\n",
        "\n",
        "\n",
        "# getting a glimpse of the dataset\n",
        "selected_book_reviews.printSchema()\n",
        "selected_book_reviews.cache()\n",
        "selected_book_reviews.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghbg-D6B4wTs",
        "outputId": "3a2aa0e1-b3c0-49ef-990f-bd8f5924bdcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of reviews: 3000000\n"
          ]
        }
      ],
      "source": [
        "# how many rows do we have in the dataset?\n",
        "print(f\"Total number of reviews: {selected_book_reviews.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr_07bsU494E",
        "outputId": "98e9d59e-8a12-4f37-99fd-658aec33734a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+-------+------------+\n",
            "| Id|Title|User_id|review/score|\n",
            "+---+-----+-------+------------+\n",
            "|  0|  208| 561787|           0|\n",
            "+---+-----+-------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do we have any missing values in the columns we selected?\n",
        "selected_book_reviews.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in selected_book_reviews.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aSIurjfZ58xV"
      },
      "outputs": [],
      "source": [
        "# we drop the rows with missing User_id values\n",
        "selected_book_reviews = selected_book_reviews.na.drop(subset=[\"User_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oNyknek6JI0",
        "outputId": "0dcf4692-4ae4-47a0-d24f-5cd78b1a4a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+-------+------------+\n",
            "| Id|Title|User_id|review/score|\n",
            "+---+-----+-------+------------+\n",
            "|  0|    0|      0|           0|\n",
            "+---+-----+-------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# we need to fill the missing values in the Title column with a placeholder - unknown\n",
        "selected_book_reviews = selected_book_reviews.fillna(\"unknown\", subset=[\"Title\"])\n",
        "\n",
        "# let's check again for missing values\n",
        "selected_book_reviews.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in selected_book_reviews.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_os5quZ6mm0",
        "outputId": "28a28736-00ba-4c14-a455-5b99df319463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate reviews (User_id, Book_id):\n",
            "+--------------+----------+-----+\n",
            "|       User_id|        Id|count|\n",
            "+--------------+----------+-----+\n",
            "| AWLFVCT9128JV|B000PBZH5M|    2|\n",
            "|A3LN7H4OQS176F|B000PDFO2Q|    2|\n",
            "|A391X5MVRLB5WR|B000N63OYK|    2|\n",
            "|A3A6575YNHTF7X|0333168968|    2|\n",
            "|A1F0YPF7GDUVT1|B000870E8S|    2|\n",
            "+--------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Number of user-book pairs with duplicate reviews: 31159\n"
          ]
        }
      ],
      "source": [
        "# we wanna check to see if we have any duplicate reviews by the same user for the same book Id?\n",
        "duplicate_reviews = selected_book_reviews.groupBy(\"User_id\", \"Id\").count().filter(\"count > 1\")\n",
        "\n",
        "# seeing the first few duplicate reviews\n",
        "print(\"Duplicate reviews (User_id, Book_id):\")\n",
        "duplicate_reviews.show(5)\n",
        "\n",
        "# counting the number of duplicate reviews (user-book id pairs)\n",
        "num_duplicate_reviews = duplicate_reviews.count()\n",
        "print(f\"Number of user-book pairs with duplicate reviews: {num_duplicate_reviews}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ_-hFqc7Ley",
        "outputId": "e6c05f03-9d0e-4f1b-bcf5-3ab913a72660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of reviews after removing duplicates: 2397614\n"
          ]
        }
      ],
      "source": [
        "# removing duplicates based on User_id and Id\n",
        "cleaned_reviews = selected_book_reviews.dropDuplicates([\"User_id\", \"Id\"])\n",
        "print(f\"Number of reviews after removing duplicates: {cleaned_reviews.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqUebRuORAm-",
        "outputId": "8e35b797-20a1-446c-c2be-3e3c21e29951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique books: 216,023\n",
            "Unique users: 1,008,972\n",
            "+----------+--------------------+-----+\n",
            "|        Id|               Title|count|\n",
            "+----------+--------------------+-----+\n",
            "|B000IEZE3G|Harry Potter and ...| 3663|\n",
            "|B000ILIJE0|The Hobbit There ...| 3576|\n",
            "|B000NWU3I4|The Hobbitt, or t...| 3562|\n",
            "|B000PC54NG|          The Hobbit| 3540|\n",
            "|B000NWQXBA|          The Hobbit| 3535|\n",
            "+----------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# checking to see how many unique books and user Ids we have in the dataset\n",
        "unique_books = cleaned_reviews.select(\"Id\").distinct().count()\n",
        "unique_users = cleaned_reviews.select(\"User_id\").distinct().count()\n",
        "\n",
        "print(f\"Unique books: {unique_books:,}\")\n",
        "print(f\"Unique users: {unique_users:,}\")\n",
        "\n",
        "\n",
        "# See books with most reviews - we will likely see some of these books in the top lists \n",
        "cleaned_reviews.groupBy(\"Id\", \"Title\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Attention!!!** Here we notice something important. We seem to have similar books (same titles) but with different book ids. we need to test this hypothesis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we normalize the titles (at least as much as we can). <br>\n",
        "1. Make all of them lower case \n",
        "2. Remove versions such as (CD) - (Audio Book) **but we will exculde generic titles such as poems because in that case we would lose information**\n",
        "<br>\n",
        "\n",
        "The idea is that I want to see the true books by their content and not by their version. If i am ranking the books by their importance, I dont care if it was paper back or hard cover and people do tend to leave diffrent reviwes for diferent editions of the same book. <br>\n",
        "However, Perfect normalization will not be achieved in this attempt but it will make our ranking more meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique original titles: 206,712\n",
            "Unique normalized titles: 198,919\n",
            "Reduction: 7,793 (3.77%)\n",
            "\n",
            "Sample data with normalized titles:\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Title                                                                                                                                             |Title_Norm                                                                                                                            |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Pinball: The Lure of the Silver Ball                                                                                                              |pinball: the lure of the silver ball                                                                                                  |\n",
            "|Whistle                                                                                                                                           |whistle                                                                                                                               |\n",
            "|The church of apostles and martyrs                                                                                                                |the church of apostles and martyrs                                                                                                    |\n",
            "|Vesuvius Club                                                                                                                                     |vesuvius club                                                                                                                         |\n",
            "|Designing Cost-Efficient Mechanisms: Minimum Constraint Design, Designing With Commercial Components, and Topics in Design Engineering (Reference)|designing cost-efficient mechanisms: minimum constraint design, designing with commercial components, and topics in design engineering|\n",
            "|Encyclopedia Sherlockiana: An A-To-Z Guide to the World of the Great Detective                                                                    |encyclopedia sherlockiana: an a-to-z guide to the world of the great detective                                                        |\n",
            "|Survival of the Prettiest: The Science of Beauty                                                                                                  |survival of the prettiest: the science of beauty                                                                                      |\n",
            "|The World According to Washington: An Asian View                                                                                                  |the world according to washington: an asian view                                                                                      |\n",
            "|Classic Warbirds in Color                                                                                                                         |classic warbirds in color                                                                                                             |\n",
            "|Mind Training                                                                                                                                     |mind training                                                                                                                         |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# normalizing the titles\n",
        "# define a list of generic titles that we want to keep as is - i took a wild guess with this based on my experince in reading books\n",
        "# also we make it into a function because we will need it for mappings\n",
        "\n",
        "def normalize_titles(df, title_column=\"Title\"):\n",
        "\n",
        "    # Define generic titles to preserve as-is\n",
        "    generic_titles = [\n",
        "        \"poems\", \"selected poems\", \"collected poems\", \n",
        "        \"essays\", \"letters\", \"stories\", \"collected stories\",\n",
        "        \"short stories\", \"anthology\", \"complete works\",\n",
        "        \"selected works\", \"collected works\", \"memoir\",\n",
        "        \"biography\", \"autobiography\", \"diary\", \"journals\"\n",
        "    ]\n",
        "    \n",
        "    # Create UDF to check if title contains any generic terms\n",
        "    @F.udf(returnType=BooleanType())\n",
        "    def contains_generic_title(title):\n",
        "        if title is None:\n",
        "            return False\n",
        "        for generic in generic_titles:\n",
        "            # Check if title contains generic phrase as a whole word\n",
        "            if re.search(r'\\b' + re.escape(generic) + r'\\b', title):\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    # First lowercase all titles\n",
        "    df_with_lowercase = df.withColumn(\n",
        "        \"LowercasedTitle\", \n",
        "        F.lower(F.col(title_column))\n",
        "    )\n",
        "    \n",
        "    # Apply normalization\n",
        "    df_normalized = df_with_lowercase.withColumn(\n",
        "        \"Title_Norm\",  # Using a consistent column name\n",
        "        F.when(\n",
        "            contains_generic_title(F.col(\"LowercasedTitle\")),\n",
        "            F.col(\"LowercasedTitle\")\n",
        "        ).otherwise(\n",
        "            F.trim(F.regexp_replace(\n",
        "                F.col(\"LowercasedTitle\"), \n",
        "                r\"\\[.*?\\]|\\(.*?\\)|audiobook|unabridged|cd\", \n",
        "                \"\"\n",
        "            ))\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Drop the intermediate column to keep the DataFrame clean\n",
        "    df_normalized = df_normalized.drop(\"LowercasedTitle\")\n",
        "    \n",
        "    return df_normalized\n",
        "\n",
        "\n",
        "# Apply the normalization function to cleaned_reviews\n",
        "cleaned_reviews_norm = normalize_titles(cleaned_reviews)\n",
        "\n",
        "# Count unique values in both columns\n",
        "orig_count = cleaned_reviews_norm.select(\"Title\").distinct().count()\n",
        "norm_count = cleaned_reviews_norm.select(\"Title_Norm\").distinct().count()\n",
        "\n",
        "print(f\"Unique original titles: {orig_count:,}\")\n",
        "print(f\"Unique normalized titles: {norm_count:,}\")\n",
        "\n",
        "# what percent of titles were reduced?\n",
        "print(f\"Reduction: {orig_count - norm_count:,} ({(orig_count - norm_count)/orig_count*100:.2f}%)\")\n",
        "\n",
        "# show some examples to see if we did a good job\n",
        "print(\"\\nSample data with normalized titles:\")\n",
        "cleaned_reviews_norm.select(\"Title\", \"Title_Norm\").distinct().orderBy(F.rand()).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since This looks somewhat okay, we abondon the told title colomns and continue with our new colomn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename NormalizedTitle to Title_Norm\n",
        "cleaned_reviews_norm = cleaned_reviews_norm.withColumnRenamed(\"NormalizedTitle\", \"Title_Norm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned_reviews_norm = cleaned_reviews_norm.drop(\"Title\")\n",
        "cleaned_reviews_norm = cleaned_reviews_norm.drop(\"LowercasedTitle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema after replacing Title with normalized version:\n",
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- review/score: double (nullable = true)\n",
            " |-- Title_Norm: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify the change\n",
        "print(\"Schema after replacing Title with normalized version:\")\n",
        "cleaned_reviews_norm.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------------------+------------+-----------------------------------------------------------+\n",
            "|Id        |User_id              |review/score|Title_Norm                                                 |\n",
            "+----------+---------------------+------------+-----------------------------------------------------------+\n",
            "|0375765263|A005300013WHNZLTK9N6F|3.0         |cracking the ap calculus ab and bc exams, 2006-2007 edition|\n",
            "|B000GQG7D2|A00540411RKGTDNU543WS|5.0         |the hobbit                                                 |\n",
            "|B0008849SS|A008059932M4DUB2IWDB8|5.0         |seven pillars of wisdom,: a triumph                        |\n",
            "|0451521196|A00891092QIVH4W1YP46A|2.0         |wuthering heights                                          |\n",
            "|B0006AQ4LI|A00891092QIVH4W1YP46A|2.0         |wuthering heights                                          |\n",
            "+----------+---------------------+------------+-----------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_reviews_norm.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Now the next problem is that titles sometimes have different book IDs (despite being the same book) and we need to verify this before thinking of a solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1T-Ni5T8tRn",
        "outputId": "f519b291-4316-45c9-b7e7-9d4fff5b6cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of titles with the same title but different IDs:\n",
            "12121\n",
            "\n",
            "Top five books with the same title and different IDs ordered by the count of distinct ids:\n",
            "+----------------------------+-----------------+\n",
            "|Title_Norm                  |distinct_id_count|\n",
            "+----------------------------+-----------------+\n",
            "|emma                        |20               |\n",
            "|persuasion                  |18               |\n",
            "|wuthering heights           |17               |\n",
            "|great expectations          |16               |\n",
            "|selected poems              |15               |\n",
            "|jane eyre                   |14               |\n",
            "|poems                       |14               |\n",
            "|the white company           |13               |\n",
            "|systematic theology         |13               |\n",
            "|sermons on several occasions|13               |\n",
            "+----------------------------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# grouping books by Title and count how many distinct IDs exist for each title\n",
        "title_id_counts = cleaned_reviews_norm.groupBy(\"Title_Norm\").agg(F.countDistinct(\"Id\").alias(\"distinct_id_count\"))\n",
        "\n",
        "# Filter titles with more than one distinct ID\n",
        "titles_with_same_name_diff_id = title_id_counts.filter(col(\"distinct_id_count\") > 1)\n",
        "\n",
        "print(\"Number of titles with the same title but different IDs:\")\n",
        "duplicate_titles = titles_with_same_name_diff_id.count()\n",
        "print(duplicate_titles)\n",
        "\n",
        "# top ten books with the most distinct IDs - same title\n",
        "print(\"\\nTop five books with the same title and different IDs ordered by the count of distinct ids:\")\n",
        "titles_with_same_name_diff_id.orderBy(col(\"distinct_id_count\").desc()).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the purpose of truly having distincs Human - Book reviews and also easier handling in the functions, we will create integer indexes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+----------+--------------------+-------+-----+\n",
            "|             User_id|user_id_int|        Id|          Title_Norm|book_id|score|\n",
            "+--------------------+-----------+----------+--------------------+-------+-----+\n",
            "|A00274963RTZUW5BU...|          6|B0006Y8M7S|how to win friend...|  72106|  5.0|\n",
            "|A00274963RTZUW5BU...|          6|B00086Q244|how to win friend...|  72106|  5.0|\n",
            "|A00540411RKGTDNU5...|         17|B000NWU3I4|the hobbitt, or t...| 158532|  5.0|\n",
            "|A00540411RKGTDNU5...|         17|B000GQG7D2|          the hobbit| 158528|  5.0|\n",
            "|A00540411RKGTDNU5...|         17|B000NWQXBA|          the hobbit| 158528|  5.0|\n",
            "|A00540411RKGTDNU5...|         17|B000NDSX6C|          the hobbit| 158528|  5.0|\n",
            "|A00540411RKGTDNU5...|         17|B000H9R1Q0|          the hobbit| 158528|  5.0|\n",
            "|A00540411RKGTDNU5...|         17|B000PC54NG|          the hobbit| 158528|  5.0|\n",
            "|A00540411RKGTDNU5...|         17|B000ILIJE0|the hobbit there ...| 158530|  5.0|\n",
            "|A00540411RKGTDNU5...|         17|B000Q032UY|the hobbit or the...| 158529|  5.0|\n",
            "+--------------------+-----------+----------+--------------------+-------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "def add_generated_ids_norm(df):\n",
        "\n",
        "    title_window = Window.orderBy(\"Title_Norm\")\n",
        "    titles_with_id = df.select(\"Title_Norm\").distinct() \\\n",
        "                      .withColumn(\"book_id\", F.dense_rank().over(title_window))\n",
        "\n",
        "    user_window = Window.orderBy(\"User_id\")\n",
        "    users_with_id = df.select(\"User_id\").distinct() \\\n",
        "                     .withColumn(\"user_id_int\", F.dense_rank().over(user_window))\n",
        "\n",
        "    result = df.join(titles_with_id, \"Title_Norm\") \\\n",
        "               .join(users_with_id, \"User_id\") \\\n",
        "               .select(\"User_id\", \"user_id_int\", \"Id\", \"Title_Norm\", \"book_id\", \n",
        "                       F.col(\"review/score\").cast(\"double\").alias(\"score\"))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "df_with_ids = add_generated_ids_norm(cleaned_reviews_norm)\n",
        "df_with_ids.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking to see if there is any collision?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Book collisions: 0\n",
            "User collisions: 0\n"
          ]
        }
      ],
      "source": [
        "book_collisions = df_with_ids.select(\"book_id\", \"Title_Norm\").distinct().groupBy(\"book_id\").count().filter(F.col(\"count\") > 1).count()\n",
        "user_collisions = df_with_ids.select(\"user_id_int\", \"User_id\").distinct().groupBy(\"user_id_int\").count().filter(F.col(\"count\") > 1).count()\n",
        "\n",
        "print(f\"Book collisions: {book_collisions}\")\n",
        "print(f\"User collisions: {user_collisions}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+--------------------+-----+\n",
            "|user_id_int|book_id|          Title_Norm|score|\n",
            "+-----------+-------+--------------------+-----+\n",
            "|          6|  72106|how to win friend...|  5.0|\n",
            "|          6|  72106|how to win friend...|  5.0|\n",
            "|         17| 158532|the hobbitt, or t...|  5.0|\n",
            "|         17| 158528|          the hobbit|  5.0|\n",
            "|         17| 158528|          the hobbit|  5.0|\n",
            "|         17| 158528|          the hobbit|  5.0|\n",
            "|         17| 158528|          the hobbit|  5.0|\n",
            "|         17| 158528|          the hobbit|  5.0|\n",
            "|         17| 158530|the hobbit there ...|  5.0|\n",
            "|         17| 158529|the hobbit or the...|  5.0|\n",
            "+-----------+-------+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# only keeping what we need \n",
        "indexed_data = df_with_ids.select(\"user_id_int\", \"book_id\", \"Title_Norm\", \"score\")\n",
        "indexed_data.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see that we run into more duplicate human-book pairs which makes sense since we basically turned many different book variations into one id, so we remove duplicates again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original count: 2397614\n",
            "After removing duplicates: 1931845\n",
            "Duplicates removed: 465769\n"
          ]
        }
      ],
      "source": [
        "# Remove duplicate user-book pairs\n",
        "indexed_data_unique = indexed_data.dropDuplicates([\"user_id_int\", \"book_id\"])\n",
        "\n",
        "a = indexed_data.count()\n",
        "b = indexed_data_unique.count()\n",
        "\n",
        "print(\"Original count:\", a)\n",
        "print(\"After removing duplicates:\", b)\n",
        "print(\"Duplicates removed:\", a-b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+--------------------+-----+\n",
            "|user_id_int|book_id|          Title_Norm|score|\n",
            "+-----------+-------+--------------------+-----+\n",
            "|          1| 181419|   this calder range|  5.0|\n",
            "|          9| 146495|the breadman's he...|  5.0|\n",
            "|         37|  79182|jane eyre / wuthe...|  5.0|\n",
            "|         45|  80657|johnny tremain: a...|  5.0|\n",
            "|         93|  39854| dawn in eclipse bay|  3.0|\n",
            "+-----------+-------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexed_data_unique.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique books: 198,919\n",
            "Number of unique users: 1,008,972\n",
            "Average reviews per user: 1.91\n",
            "+-----------+---+---+---+-----------+\n",
            "|min_reviews| Q1| Q2| Q3|max_reviews|\n",
            "+-----------+---+---+---+-----------+\n",
            "|          1|1.0|1.0|1.0|       5243|\n",
            "+-----------+---+---+---+-----------+\n",
            "\n",
            "\n",
            "Top 10 Most Active Reviewers:\n",
            "+-----------+-----+\n",
            "|user_id_int|count|\n",
            "+-----------+-----+\n",
            "|      35080| 5243|\n",
            "|     859798| 3297|\n",
            "|     870831| 1524|\n",
            "|     246677| 1374|\n",
            "|     208408| 1071|\n",
            "|     148621| 1060|\n",
            "|     448655|  859|\n",
            "|     164799|  779|\n",
            "|     695950|  713|\n",
            "|      96987|  684|\n",
            "+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Count unique books\n",
        "unique_books = indexed_data_unique.select(\"book_id\").distinct().count()\n",
        "\n",
        "# Count unique users\n",
        "unique_users = indexed_data_unique.select(\"user_id_int\").distinct().count()\n",
        "\n",
        "# Calculate average reviews per user\n",
        "reviews_per_user = indexed_data_unique.groupBy(\"user_id_int\").count()\n",
        "avg_reviews_per_user = reviews_per_user.select(F.avg(\"count\")).first()[0]\n",
        "\n",
        "# Print the statistics\n",
        "print(f\"Number of unique books: {unique_books:,}\")\n",
        "print(f\"Number of unique users: {unique_users:,}\")\n",
        "print(f\"Average reviews per user: {avg_reviews_per_user:.2f}\")\n",
        "\n",
        "# Additional distribution statistics\n",
        "reviews_distribution = reviews_per_user.select(\n",
        "    F.min(\"count\").alias(\"min_reviews\"),\n",
        "    F.expr(\"percentile(count, 0.25)\").alias(\"Q1\"),\n",
        "    F.expr(\"percentile(count, 0.5)\").alias(\"Q2\"),\n",
        "    F.expr(\"percentile(count, 0.75)\").alias(\"Q3\"),\n",
        "    F.max(\"count\").alias(\"max_reviews\")\n",
        ")\n",
        "\n",
        "reviews_distribution.show()\n",
        "\n",
        "# Show top 10 most active reviewers\n",
        "print(\"\\nTop 10 Most Active Reviewers:\")\n",
        "reviews_per_user.orderBy(F.col(\"count\").desc()).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ATTENTION** <br>\n",
        "Because during debugging the spark session would often crash and i had to restart the kernel, i saved the final data so each time that i would re-run the spark, i wouldnt have to re-run the data processing too. <br>\n",
        "be careful this snippet of code will make and save inside the relative directory. You might wanna skip the next 2 blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved DataFrame with 1931845 rows\n"
          ]
        }
      ],
      "source": [
        "# Convert to pandas DataFrame for reliable local storage\n",
        "final_data = indexed_data_unique  # Ensure we use the right DataFrame\n",
        "pandas_df = final_data.toPandas()\n",
        "\n",
        "# Create directory if needed\n",
        "os.makedirs(\"pandas_data\", exist_ok=True)\n",
        "\n",
        "# Save as CSV using pandas (no Hadoop dependency)\n",
        "pandas_df.to_csv(\"pandas_data/final_data.csv\", index=False)\n",
        "\n",
        "# Also save as pickle for faster reloading\n",
        "pandas_df.to_pickle(\"pandas_data/final_data.pkl\")\n",
        "\n",
        "print(f\"Successfully saved DataFrame with {len(pandas_df)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ONLY USE IF THE SPARK CRASHES AND YOU ARE TRYING TO RESTART THE SESSION. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Read the final data back from the saved files\n",
        "\n",
        "final_data = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"inferSchema\", \"true\") \\\n",
        "        .csv(\"pandas_data/final_data.csv\")\n",
        "final_data.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PHASE ONE (1.1): MAPPINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created mapping for 198919 unique books\n",
            "\n",
            "Sample title mappings:\n",
            "Book ID: 9436 ---> Title: aftershocks\n",
            "Book ID: 120026 ---> Title: rats\n",
            "Book ID: 162847 ---> Title: the lost continent\n",
            "Book ID: 70848 ---> Title: how to argue and win every time\n"
          ]
        }
      ],
      "source": [
        "# title - book id\n",
        "def create_title_mapping(df):\n",
        "    # Select only the needed columns and get distinct pairs\n",
        "    title_rows = df.select(\"book_id\", \"Title_Norm\").distinct().collect()\n",
        "    \n",
        "    # Convert to dictionary\n",
        "    title_mapping = {row[\"book_id\"]: row[\"Title_Norm\"] for row in title_rows}\n",
        "    \n",
        "    print(f\"Created mapping for {len(title_mapping)} unique books\")\n",
        "    return title_mapping\n",
        "\n",
        "# Create the mapping\n",
        "title_mapping = create_title_mapping(final_data)\n",
        "\n",
        "# Preview a few entries\n",
        "print(\"\\nSample title mappings:\")\n",
        "for book_id, title in list(title_mapping.items())[:4]:\n",
        "    print(f\"Book ID: {book_id} ---> Title: {title}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created rating mapping for 198919 books\n",
            "\n",
            "Sample book rating mappings:\n",
            "Book ID: 120706 ---> Rating: 4.39/5 (442 reviews)\n",
            "Book ID: 103357 ---> Rating: 5.00/5 (1 reviews)\n",
            "Book ID: 89476 ---> Rating: 3.87/5 (231 reviews)\n",
            "Book ID: 18654 ---> Rating: 3.93/5 (41 reviews)\n",
            "Book ID: 7754 ---> Rating: 5.00/5 (6 reviews)\n"
          ]
        }
      ],
      "source": [
        "# average rating - book id\n",
        "def create_book_rating_mapping(df):\n",
        "    # Group by book_id and calculate average score\n",
        "    avg_ratings = df.groupBy(\"book_id\") \\\n",
        "                    .agg(F.avg(\"score\").alias(\"avg_rating\"),\n",
        "                         F.count(\"score\").alias(\"num_ratings\")) \\\n",
        "                    .collect()\n",
        "    \n",
        "    # Convert to dictionary\n",
        "    rating_mapping = {row[\"book_id\"]: {\n",
        "                          \"avg_rating\": float(row[\"avg_rating\"]),\n",
        "                          \"num_ratings\": row[\"num_ratings\"]\n",
        "                      } for row in avg_ratings}\n",
        "    \n",
        "    print(f\"Created rating mapping for {len(rating_mapping)} books\")\n",
        "    return rating_mapping\n",
        "\n",
        "book_rating_mapping = create_book_rating_mapping(final_data)\n",
        "\n",
        "# Preview a few entries\n",
        "print(\"\\nSample book rating mappings:\")\n",
        "sample_books = list(book_rating_mapping.keys())[:5]\n",
        "for book_id in sample_books:\n",
        "    rating_info = book_rating_mapping[book_id]\n",
        "    print(f\"Book ID: {book_id} ---> Rating: {rating_info['avg_rating']:.2f}/5 ({rating_info['num_ratings']} reviews)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need genres for two thing:\n",
        "1. showing the genre of top books across all models.\n",
        "2. making topic sensitive pagerank\n",
        "<br>\n",
        "\n",
        "\n",
        "First we read it from the second csv file and we normalize its titles for comparing against ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded metadata for 165975 books with genres\n",
            "Created genre mapping for 198919 books\n",
            "Matched genres: 161574 (81.2%)\n",
            "\n",
            "Sample genre mappings:\n",
            "Book ID: 9436 ---> Genre: ['Fiction']\n",
            "Book ID: 120026 ---> Genre: ['Art']\n",
            "Book ID: 162847 ---> Genre: ['Union catalogs']\n",
            "Book ID: 70848 ---> Genre: ['Language Arts & Disciplines']\n"
          ]
        }
      ],
      "source": [
        "# book genre - book id\n",
        "\n",
        "book_data = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"quote\", '\"') \\\n",
        "    .option(\"escape\", '\"') \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .csv(\"D:/AMD/data/Books_data.csv\") \\\n",
        "    .select(\"Title\", \"categories\")  \n",
        "\n",
        "book_data_norm = normalize_titles(book_data)\n",
        "\n",
        "genre_by_title = {row[\"Title_Norm\"]: row[\"categories\"] \n",
        "                  for row in book_data_norm.select(\"Title_Norm\", \"categories\").collect() \n",
        "                  if row[\"categories\"] is not None}\n",
        "\n",
        "print(f\"Loaded metadata for {len(genre_by_title)} books with genres\")\n",
        "\n",
        "def create_genre_mapping(df):\n",
        "    books = df.select(\"book_id\", \"Title_Norm\").distinct().collect()\n",
        "    \n",
        "    genre_mapping = {row[\"book_id\"]: genre_by_title.get(row[\"Title_Norm\"], \"Unknown\") \n",
        "                    for row in books}\n",
        "    \n",
        "    matched = sum(1 for genre in genre_mapping.values() if genre != \"Unknown\")\n",
        "    print(f\"Created genre mapping for {len(genre_mapping)} books\")\n",
        "    print(f\"Matched genres: {matched} ({matched/len(genre_mapping)*100:.1f}%)\")\n",
        "    \n",
        "    return genre_mapping\n",
        "\n",
        "genre_mapping = create_genre_mapping(final_data)\n",
        "\n",
        "# Preview a few entries\n",
        "print(\"\\nSample genre mappings:\")\n",
        "for book_id in list(genre_mapping.keys())[:4]:\n",
        "    title = title_mapping.get(book_id, \"Unknown\")\n",
        "    genre = genre_mapping[book_id]\n",
        "    print(f\"Book ID: {book_id} ---> Genre: {genre}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What genres are most common? You can use this info for gerring topic sensitive pagerank later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 genres by book count:\n",
            "1. Unknown: 37345 books\n",
            "2. ['Fiction']: 21731 books\n",
            "3. ['Religion']: 8995 books\n",
            "4. ['History']: 8874 books\n",
            "5. ['Juvenile Fiction']: 6267 books\n",
            "6. ['Biography & Autobiography']: 6037 books\n",
            "7. ['Business & Economics']: 5410 books\n",
            "8. ['Computers']: 4173 books\n",
            "9. ['Social Science']: 3552 books\n",
            "10. ['Juvenile Nonfiction']: 3221 books\n"
          ]
        }
      ],
      "source": [
        "# Get the top 10 genres\n",
        "genre_counts = {}\n",
        "for genre in genre_mapping.values():\n",
        "    genre_counts[genre] = genre_counts.get(genre, 0) + 1\n",
        "\n",
        "sorted_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 genres by book count:\")\n",
        "for i, (genre, count) in enumerate(sorted_genres[:10]):\n",
        "    print(f\"{i+1}. {genre}: {count} books\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **PHASE TWO: BIULDING THE GRAPH**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24-t4hPX6PI"
      },
      "source": [
        "### What is the Graph logic?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDDZKWwUX3p"
      },
      "source": [
        "Writing a fucntion that makes the graph - Books connected if they share 2 or more reviews from at least 2 unique people. <br>\n",
        "BOOKA <--> BOOKB if USER-A and USER-B reviewed both A and B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One more think to point out is that pagerank needs directed graph and since our book network has no direction, we have to double each link and make it **bidirectional**.\n",
        "<br>\n",
        "BOOK1 -> BOOK2\n",
        "BOOK1 <- BOOK2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "\n",
        "def build_book_graph(df, threshold, purpose):\n",
        "    # 1. Prepare user-book pairs (already unique)\n",
        "    user_books = df.select(\"user_id_int\", \"book_id\").cache()\n",
        "\n",
        "    # 2. Generate book-book pairs for users, count shared users (OPTIMIZED)\n",
        "    book_pairs = user_books.alias(\"ub1\").join(\n",
        "        user_books.alias(\"ub2\"),\n",
        "        F.col(\"ub1.user_id_int\") == F.col(\"ub2.user_id_int\")\n",
        "    ).filter(\n",
        "        F.col(\"ub1.book_id\") < F.col(\"ub2.book_id\")\n",
        "    ).select(\n",
        "        F.col(\"ub1.book_id\").alias(\"src\"),\n",
        "        F.col(\"ub2.book_id\").alias(\"dst\")\n",
        "    )\n",
        "\n",
        "    edges_df = book_pairs.groupBy(\"src\", \"dst\").count() \\\n",
        "        .filter(F.col(\"count\") >= threshold) \\\n",
        "        .select(\"src\", \"dst\").cache()\n",
        "\n",
        "    # 3. Make undirected (bidirectional) edges\n",
        "    edges_df = edges_df.union(edges_df.select(F.col(\"dst\").alias(\"src\"), F.col(\"src\").alias(\"dst\")))\n",
        "\n",
        "    # 4. Vertices DataFrame\n",
        "    vertices_df = edges_df.select(\"src\").union(edges_df.select(\"dst\")).distinct().withColumnRenamed(\"src\", \"id\")\n",
        "\n",
        "    # 5. Build GraphFrame\n",
        "    graph = GraphFrame(vertices_df, edges_df)\n",
        "\n",
        "    if purpose == \"pagerank_builtin\":\n",
        "        # Only return GraphFrame for built-in PageRank\n",
        "        return {\"graph\": graph}\n",
        "    elif purpose == \"pagerank_rdd\":\n",
        "        # Return RDD of edges and the number of nodes for RDD-based algorithms\n",
        "        edges_rdd = graph.edges.rdd.map(lambda row: (row[\"src\"], row[\"dst\"]))\n",
        "        nodes_count = vertices_df.count()\n",
        "        return {\"edges_rdd\": edges_rdd, \"nodes_count\": nodes_count}\n",
        "    elif purpose == \"pagerank_python\":\n",
        "        # Return local lists for pure Python algorithms (be careful with large graphs!)\n",
        "        links = [(row[\"src\"], row[\"dst\"]) for row in edges_df.collect()]\n",
        "        pages = [row[\"id\"] for row in vertices_df.collect()]\n",
        "        return {\"pages\": pages, \"links\": links}\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown purpose: {purpose}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **PHASE THREE: OBTAINING A BENCHMRK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Non personalized classic pagerak from the graphframe. Its results will be a testing ground for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you refer to the graphframe user giude you can see this: <br>\n",
        "link: https://graphframes.io/docs/_site/user-guide.html#pagerank\n",
        "\n",
        "You can see we have 2 oprions, either fixed iteration or fixed tolerance (not at the same time). I Went with fixed iterations, you can easily test the other one as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "FZG3AJ2APifm",
        "outputId": "08225c68-b77c-4d7f-e870-34bfbae43ef9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data_1 = build_book_graph(final_data, threshold=2, purpose=\"pagerank_builtin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of vertices in the graph: 47,488\n",
            "Number of edges in the graph: 4,084,494\n"
          ]
        }
      ],
      "source": [
        "# Count vertices and edges in the graph (just to know the size of the graph)\n",
        "num_vertices = data_1['graph'].vertices.count()\n",
        "num_edges = data_1['graph'].edges.count()\n",
        "\n",
        "print(f\"Number of vertices in the graph: {num_vertices:,}\")\n",
        "print(f\"Number of edges in the graph: {num_edges:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique connections (edges/2): 2,042,247\n"
          ]
        }
      ],
      "source": [
        "# since the graph was bidirectional, we can divide the number of edges by 2 to get the actual number of connections\n",
        "print(f\"Number of unique connections (edges/2): {num_edges // 2:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "edIWN_SWY1Ro",
        "outputId": "aeef7e21-0a26-4052-98e2-41c5241e04c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        }
      ],
      "source": [
        "# running the built-in PageRank algorithm in oder to get a benchmark\n",
        "# ATTENTION: This may take some time and it is slow, run this at your own risk!\n",
        "\n",
        "result_one = data_1['graph'].pageRank(resetProbability=0.15, maxIter=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First extract the top 20 book IDs - the benchmark for PageRank\n",
        "# ATTENTION: This one too will take long, run this at your own risk!\n",
        "\n",
        "top_20_books = result_one.vertices.orderBy(result_one.vertices.pagerank.desc()).limit(20).collect()\n",
        "top_20_ids = [row[\"id\"] for row in top_20_books]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 20 Books by PageRank:\n",
            "--------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "--------------------\n",
            " 1   | harry potter and the sorcerer's stone    | ['Juvenile Fiction'] |    4.69    | 56.752445\n",
            " 2   | blink: the power of thinking without ... | ['Business & Econ... |    3.70    | 41.615154\n",
            " 3   | the catcher in the rye                   | ['Young Adult Fic... |    3.92    | 41.596783\n",
            " 4   | five people you meet in heaven           | ['Fiction']          |    4.16    | 39.573220\n",
            " 5   | john adams                               | ['Electronic books'] |    4.68    | 38.860889\n",
            " 6   | night                                    | ['Juvenile Fiction'] |    4.55    | 35.996820\n",
            " 7   | the great gatsby                         | ['Fiction']          |    4.15    | 35.926736\n",
            " 8   | guns, germs, and steel: the fates of ... | ['History']          |    4.01    | 35.807039\n",
            " 9   | the tipping point: how little things ... | ['Reference']        |    4.12    | 35.586899\n",
            " 10  | great gatsby                             | ['American Dream']   |    4.16    | 35.580328\n",
            " 11  | manhattan stories from the heart of a... | ['Manhattan (New ... |    4.16    | 34.914868\n",
            " 12  | the hobbit there and back again          | ['Adventure stori... |    4.68    | 34.517521\n",
            " 13  | the hobbit                               | ['Juvenile Fiction'] |    4.68    | 34.517521\n",
            " 14  | the hobbit or there and back again       | ['Juvenile Fiction'] |    4.68    | 34.491207\n",
            " 15  | the hobbitt, or there and back again;... | ['Fiction']          |    4.68    | 34.371543\n",
            " 16  | fahrenheit 451                           | ['Comics & Graphi... |    4.23    | 33.929131\n",
            " 17  | who moved my cheese? an-amazing way t... | ['Business & Econ... |    3.41    | 33.608993\n",
            " 18  | to kill a mocking bird                   | ['Drama']            |    4.60    | 33.179958\n",
            " 19  | harper lee's to kill a mockingbird       | ['Juvenile Nonfic... |    4.61    | 33.179958\n",
            " 20  | to kill a mockingbird                    | ['Performing Arts']  |    4.59    | 33.179958\n",
            "\n",
            "Top 20 book IDs: [65953, 22785, 147537, 55057, 80380, 104081, 156969, 64688, 176571, 63473, 92527, 158530, 158528, 158529, 158532, 52254, 193670, 183029, 65872, 183030]\n"
          ]
        }
      ],
      "source": [
        "# Create a formatted table with all required information\n",
        "print(\"\\nTop 20 Books by PageRank:\")\n",
        "print(\"-\" * 20)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "for i, row in enumerate(top_20_books):\n",
        "    book_id = row[\"id\"]\n",
        "    pagerank_score = row[\"pagerank\"]\n",
        "    \n",
        "    # Get metadata from your mappings\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "    \n",
        "    # Get genre - limited to 20 chars\n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "    \n",
        "    # Get rating info\n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "    \n",
        "    # Print formatted row\n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")\n",
        "\n",
        "print(f\"\\nTop 20 book IDs: {top_20_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above list is our benchmark, ideally we want the result of our following models to be identical or at least very close to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **PHASE FOUR: ATTEMPTING TO WRITE THE ALGORITHM FROM SCRATCH**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we write a function using only python, just to see if we can implement this logic ourselves or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pagerank_python(pages, links, beta=0.85, max_iter=100, tol=1e-6, silent=False):\n",
        "\n",
        "    if not pages:\n",
        "        raise ValueError(\"Pages list cannot be empty\")\n",
        "    if not links:\n",
        "        raise ValueError(\"Links list cannot be empty\")\n",
        "    if not (0 <= beta <= 1):\n",
        "        raise ValueError(f\"Beta must be between 0 and 1, got {beta}\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(f\"Max iterations must be positive, got {max_iter}\")\n",
        "    if tol <= 0:\n",
        "        raise ValueError(f\"Tolerance must be positive, got {tol}\")\n",
        "    \n",
        "    N = len(pages)\n",
        "    if not silent:\n",
        "        print(f\"Starting PageRank with {N:,} pages\")\n",
        "    \n",
        "    ranks = {p: 1.0 / N for p in pages}\n",
        "    initial_sum = sum(ranks.values())\n",
        "    if not silent:\n",
        "        print(f\"Initialized ranks (sum={initial_sum:.8f})\")\n",
        "    \n",
        "    # Step 2: Build adjacency list\n",
        "    adjacency = defaultdict(list)\n",
        "    for src, dst in links:\n",
        "        adjacency[src].append(dst)\n",
        "    \n",
        "    nodes_with_outlinks = len(adjacency)\n",
        "    total_edges = len(links)\n",
        "    if not silent:\n",
        "        print(f\"Built adjacency list ({nodes_with_outlinks:,} nodes with outlinks, {total_edges:,} total edges)\")\n",
        "        print(f\"Starting PageRank iterations...\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    for iteration in range(max_iter):\n",
        "        new_ranks = {p: (1 - beta) / N for p in pages}\n",
        "\n",
        "        for src in pages:\n",
        "            neighbors = adjacency.get(src, [])\n",
        "            if not neighbors:\n",
        "                continue\n",
        "            share = ranks[src] / len(neighbors)\n",
        "            for dst in neighbors:\n",
        "                new_ranks[dst] += beta * share\n",
        "\n",
        "        diff = (sum((new_ranks[p] - ranks[p])**2 for p in pages))**0.5  # L2 norm\n",
        "        rank_sum = sum(new_ranks.values())\n",
        "        \n",
        "        if not silent:\n",
        "            print(f\"Iteration {iteration + 1:3d}: L2 diff={diff:.6f}, sum={rank_sum:.8f}\")\n",
        "        \n",
        "        ranks = new_ranks\n",
        "        if diff < tol:\n",
        "            break\n",
        "\n",
        "    if not silent:\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"Converged after {iteration + 1} iterations\")\n",
        "        \n",
        "        # Final validation\n",
        "        final_sum = sum(ranks.values())\n",
        "        if abs(final_sum - 1.0) > 1e-6:\n",
        "            print(f\"Warning: Final sum ({final_sum:.8f}) deviates from 1.0\")\n",
        "        else:\n",
        "            print(f\"Rank sum validation passed\")\n",
        "    \n",
        "    return sorted(ranks.items(), key=lambda x: -x[1]), iteration + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first we make the graph for this specific function\n",
        "data_2 = build_book_graph(final_data, threshold=2, purpose=\"pagerank_python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting PageRank with 47,488 pages\n",
            "Initialized ranks (sum=1.00000000)\n",
            "Built adjacency list (47,488 nodes with outlinks, 4,084,494 total edges)\n",
            "Starting PageRank iterations...\n",
            "------------------------------------------------------------\n",
            "Iteration   1: L2 diff=0.010299, sum=1.00000000\n",
            "Iteration   2: L2 diff=0.003460, sum=1.00000000\n",
            "Iteration   3: L2 diff=0.000818, sum=1.00000000\n",
            "Iteration   4: L2 diff=0.000437, sum=1.00000000\n",
            "Iteration   5: L2 diff=0.000266, sum=1.00000000\n",
            "Iteration   6: L2 diff=0.000181, sum=1.00000000\n",
            "Iteration   7: L2 diff=0.000130, sum=1.00000000\n",
            "Iteration   8: L2 diff=0.000101, sum=1.00000000\n",
            "Iteration   9: L2 diff=0.000079, sum=1.00000000\n",
            "Iteration  10: L2 diff=0.000065, sum=1.00000000\n",
            "Iteration  11: L2 diff=0.000053, sum=1.00000000\n",
            "Iteration  12: L2 diff=0.000044, sum=1.00000000\n",
            "Iteration  13: L2 diff=0.000037, sum=1.00000000\n",
            "Iteration  14: L2 diff=0.000031, sum=1.00000000\n",
            "Iteration  15: L2 diff=0.000026, sum=1.00000000\n",
            "Iteration  16: L2 diff=0.000022, sum=1.00000000\n",
            "Iteration  17: L2 diff=0.000019, sum=1.00000000\n",
            "Iteration  18: L2 diff=0.000016, sum=1.00000000\n",
            "Iteration  19: L2 diff=0.000013, sum=1.00000000\n",
            "Iteration  20: L2 diff=0.000011, sum=1.00000000\n",
            "Iteration  21: L2 diff=0.000010, sum=1.00000000\n",
            "Iteration  22: L2 diff=0.000008, sum=1.00000000\n",
            "Iteration  23: L2 diff=0.000007, sum=1.00000000\n",
            "Iteration  24: L2 diff=0.000006, sum=1.00000000\n",
            "Iteration  25: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  26: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  27: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  28: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  29: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  30: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  31: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  32: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  33: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  34: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  35: L2 diff=0.000001, sum=1.00000000\n",
            "------------------------------------------------------------\n",
            "Converged after 35 iterations\n",
            "Rank sum validation passed\n"
          ]
        }
      ],
      "source": [
        "result_two = pagerank_python(data_2['pages'], data_2['links'], beta=0.85, max_iter=100, tol=1e-6, silent=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 20 Books by Custom PageRank (converged in 35 iterations):\n",
            "------------------------------------------------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1   | harry potter and the sorcerer's stone    | ['Juvenile Fiction'] |    4.69    | 0.001195\n",
            " 2   | blink: the power of thinking without ... | ['Business & Econ... |    3.70    | 0.000876\n",
            " 3   | the catcher in the rye                   | ['Young Adult Fic... |    3.92    | 0.000876\n",
            " 4   | five people you meet in heaven           | ['Fiction']          |    4.16    | 0.000833\n",
            " 5   | john adams                               | ['Electronic books'] |    4.68    | 0.000818\n",
            " 6   | night                                    | ['Juvenile Fiction'] |    4.55    | 0.000758\n",
            " 7   | the great gatsby                         | ['Fiction']          |    4.15    | 0.000757\n",
            " 8   | guns, germs, and steel: the fates of ... | ['History']          |    4.01    | 0.000754\n",
            " 9   | the tipping point: how little things ... | ['Reference']        |    4.12    | 0.000749\n",
            " 10  | great gatsby                             | ['American Dream']   |    4.16    | 0.000749\n",
            " 11  | manhattan stories from the heart of a... | ['Manhattan (New ... |    4.16    | 0.000735\n",
            " 12  | the hobbit                               | ['Juvenile Fiction'] |    4.68    | 0.000727\n",
            " 13  | the hobbit there and back again          | ['Adventure stori... |    4.68    | 0.000727\n",
            " 14  | the hobbit or there and back again       | ['Juvenile Fiction'] |    4.68    | 0.000726\n",
            " 15  | the hobbitt, or there and back again;... | ['Fiction']          |    4.68    | 0.000724\n",
            " 16  | fahrenheit 451                           | ['Comics & Graphi... |    4.23    | 0.000715\n",
            " 17  | who moved my cheese? an-amazing way t... | ['Business & Econ... |    3.41    | 0.000708\n",
            " 18  | harper lee's to kill a mockingbird       | ['Juvenile Nonfic... |    4.61    | 0.000699\n",
            " 19  | to kill a mockingbird                    | ['Performing Arts']  |    4.59    | 0.000699\n",
            " 20  | to kill a mocking bird                   | ['Drama']            |    4.60    | 0.000699\n",
            "\n",
            "Top 20 book IDs from custom PageRank: [65953, 22785, 147537, 55057, 80380, 104081, 156969, 64688, 176571, 63473, 92527, 158528, 158530, 158529, 158532, 52254, 193670, 65872, 183030, 183029]\n"
          ]
        }
      ],
      "source": [
        "# TOP 20 Books by Custom PageRank - PURE PYTHON implementation\n",
        "\n",
        "\n",
        "custom_pagerank_sorted, iterations_to_converge = result_two  \n",
        "custom_pagerank_top20 = custom_pagerank_sorted[:20]  \n",
        "custom_pagerank_ids = [book_id for book_id, score in custom_pagerank_top20]  \n",
        "\n",
        "# Create a formatted table with all required information\n",
        "print(f\"\\nTop 20 Books by Custom PageRank (converged in {iterations_to_converge} iterations):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (book_id, pagerank_score) in enumerate(custom_pagerank_top20):\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "    \n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "    \n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "    \n",
        "    # Print formatted row\n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")\n",
        "\n",
        "print(f\"\\nTop 20 book IDs from custom PageRank: {custom_pagerank_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **PHASE FOUR (4.1): USING SPARK RDD FOR PAGERNAK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_3 = build_book_graph(final_data, threshold=2, purpose=\"pagerank_rdd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pagerank_rdd(edges_rdd, nodes_count, damping_factor=0.85, max_iter=100, tolerance=1e-6, silent=False):\n",
        "\n",
        "    \n",
        "    # Input validation\n",
        "    if not edges_rdd:\n",
        "        raise ValueError(\"Edges RDD cannot be empty\")\n",
        "    if nodes_count <= 0:\n",
        "        raise ValueError(f\"Nodes count must be positive, got {nodes_count}\")\n",
        "    if not (0 <= damping_factor <= 1):\n",
        "        raise ValueError(f\"Damping factor must be between 0 and 1, got {damping_factor}\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(f\"Max iterations must be positive, got {max_iter}\")\n",
        "    if tolerance <= 0:\n",
        "        raise ValueError(f\"Tolerance must be positive, got {tolerance}\")\n",
        "    \n",
        "    if not silent:\n",
        "        print(f\"Starting PageRank RDD with {nodes_count:,} nodes\")\n",
        "    \n",
        "    if not silent:\n",
        "        print(\"Building adjacency list...\")\n",
        "    adjacency_rdd = edges_rdd.groupByKey().mapValues(list).cache()\n",
        "    if not silent:\n",
        "        print(\"adjacency list built and cached\")\n",
        "    \n",
        "    # Identify all nodes in the graph\n",
        "    if not silent:\n",
        "        print(\"Identifying all nodes in the graph...\")\n",
        "    all_nodes = edges_rdd.flatMap(lambda x: [x[0], x[1]]).distinct().collect()\n",
        "    \n",
        "    teleport_prob = (1 - damping_factor) / len(all_nodes)\n",
        "    if not silent:\n",
        "        print(f\"Teleportation probability set: {teleport_prob:.8f}\")\n",
        "    \n",
        "    ranks = {node: 1.0 / len(all_nodes) for node in all_nodes}\n",
        "    initial_sum = sum(ranks.values())\n",
        "    if not silent:\n",
        "        print(f\"Initial ranks set (sum={initial_sum:.8f})\")\n",
        "        print(f\"Starting PageRank iterations...\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    for iteration in range(max_iter):\n",
        "        old_ranks = ranks.copy()\n",
        "        \n",
        "        ranks_bc = edges_rdd.context.broadcast(ranks)\n",
        "        \n",
        "        contributions = adjacency_rdd.flatMap(\n",
        "            lambda node_neighbors: [\n",
        "                (neighbor, damping_factor * ranks_bc.value[node_neighbors[0]] / len(node_neighbors[1]))\n",
        "                for neighbor in node_neighbors[1]\n",
        "            ]\n",
        "        ).reduceByKey(lambda a, b: a + b).collectAsMap()\n",
        "        \n",
        "        ranks = {node: teleport_prob + contributions.get(node, 0) for node in all_nodes}\n",
        "        \n",
        "        diff = sum((ranks[node] - old_ranks[node])**2 for node in all_nodes)**0.5 # L2 norm\n",
        "        \n",
        "        total_sum = sum(ranks.values())\n",
        "        \n",
        "        if not silent:\n",
        "            print(f\"Iteration {iteration + 1:3d}: L2 diff={diff:.6f}, sum={total_sum:.8f}\")\n",
        "        \n",
        "        ranks_bc.unpersist()\n",
        "        \n",
        "        if diff < tolerance:\n",
        "            if not silent:\n",
        "                print(\"-\" * 60)\n",
        "                print(f\"Converged after {iteration + 1} iterations\")\n",
        "            break\n",
        "    \n",
        "    if not silent:\n",
        "        if iteration + 1 == max_iter:\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"Reached maximum iterations ({max_iter}) without full convergence\")\n",
        "        \n",
        "        print(f\"Final rank sum: {sum(ranks.values()):.8f}\")\n",
        "        \n",
        "        # Final validation\n",
        "        final_sum = sum(ranks.values())\n",
        "        if abs(final_sum - 1.0) > 1e-6:\n",
        "            print(f\"Warning: Final sum ({final_sum:.8f}) deviates from 1.0\")\n",
        "        else:\n",
        "            print(f\"Rank sum validation passed\")\n",
        "    \n",
        "    adjacency_rdd.unpersist()\n",
        "    \n",
        "    # Return sorted ranks and the number of iterations performed\n",
        "    return sorted(ranks.items(), key=lambda x: -x[1]), (iteration + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "carefull, this takes longer than the python version to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "edges_rdd partitions: 400\n",
            "edges_rdd is cached: False\n",
            "Starting PageRank RDD with 47,488 nodes\n",
            "Building adjacency list...\n",
            "adjacency list built and cached\n",
            "Identifying all nodes in the graph...\n",
            "Teleportation probability set: 0.00000316\n",
            "Initial ranks set (sum=1.00000000)\n",
            "Starting PageRank iterations...\n",
            "------------------------------------------------------------\n",
            "Iteration   1: L2 diff=0.010299, sum=1.00000000\n",
            "Iteration   2: L2 diff=0.003460, sum=1.00000000\n",
            "Iteration   3: L2 diff=0.000818, sum=1.00000000\n",
            "Iteration   4: L2 diff=0.000437, sum=1.00000000\n",
            "Iteration   5: L2 diff=0.000266, sum=1.00000000\n",
            "Iteration   6: L2 diff=0.000181, sum=1.00000000\n",
            "Iteration   7: L2 diff=0.000130, sum=1.00000000\n",
            "Iteration   8: L2 diff=0.000101, sum=1.00000000\n",
            "Iteration   9: L2 diff=0.000079, sum=1.00000000\n",
            "Iteration  10: L2 diff=0.000065, sum=1.00000000\n",
            "Iteration  11: L2 diff=0.000053, sum=1.00000000\n",
            "Iteration  12: L2 diff=0.000044, sum=1.00000000\n",
            "Iteration  13: L2 diff=0.000037, sum=1.00000000\n",
            "Iteration  14: L2 diff=0.000031, sum=1.00000000\n",
            "Iteration  15: L2 diff=0.000026, sum=1.00000000\n",
            "Iteration  16: L2 diff=0.000022, sum=1.00000000\n",
            "Iteration  17: L2 diff=0.000019, sum=1.00000000\n",
            "Iteration  18: L2 diff=0.000016, sum=1.00000000\n",
            "Iteration  19: L2 diff=0.000013, sum=1.00000000\n",
            "Iteration  20: L2 diff=0.000011, sum=1.00000000\n",
            "Iteration  21: L2 diff=0.000010, sum=1.00000000\n",
            "Iteration  22: L2 diff=0.000008, sum=1.00000000\n",
            "Iteration  23: L2 diff=0.000007, sum=1.00000000\n",
            "Iteration  24: L2 diff=0.000006, sum=1.00000000\n",
            "Iteration  25: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  26: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  27: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  28: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  29: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  30: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  31: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  32: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  33: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  34: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  35: L2 diff=0.000001, sum=1.00000000\n",
            "------------------------------------------------------------\n",
            "Converged after 35 iterations\n",
            "Final rank sum: 1.00000000\n",
            "Rank sum validation passed\n"
          ]
        }
      ],
      "source": [
        "# Extract the edges_rdd and nodes_count from your graph\n",
        "edges_rdd = data_3[\"edges_rdd\"]\n",
        "nodes_count = data_3[\"nodes_count\"]\n",
        "\n",
        "# Check partitioning\n",
        "print(f\"edges_rdd partitions: {edges_rdd.getNumPartitions()}\")\n",
        "print(f\"edges_rdd is cached: {edges_rdd.is_cached}\")\n",
        "\n",
        "edges_rdd_fixed = edges_rdd.repartition(8).cache()\n",
        "edges_rdd_fixed.count()  # Force caching\n",
        "\n",
        "# Run PageRank with your function\n",
        "ranks, iterations = pagerank_rdd(edges_rdd_fixed, nodes_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 20 Books by RDD PageRank (converged in 35 iterations):\n",
            "------------------------------------------------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1   | harry potter and the sorcerer's stone    | ['Juvenile Fiction'] |    4.69    | 0.001195\n",
            " 2   | blink: the power of thinking without ... | ['Business & Econ... |    3.70    | 0.000876\n",
            " 3   | the catcher in the rye                   | ['Young Adult Fic... |    3.92    | 0.000876\n",
            " 4   | five people you meet in heaven           | ['Fiction']          |    4.16    | 0.000833\n",
            " 5   | john adams                               | ['Electronic books'] |    4.68    | 0.000818\n",
            " 6   | night                                    | ['Juvenile Fiction'] |    4.55    | 0.000758\n",
            " 7   | the great gatsby                         | ['Fiction']          |    4.15    | 0.000757\n",
            " 8   | guns, germs, and steel: the fates of ... | ['History']          |    4.01    | 0.000754\n",
            " 9   | the tipping point: how little things ... | ['Reference']        |    4.12    | 0.000749\n",
            " 10  | great gatsby                             | ['American Dream']   |    4.16    | 0.000749\n",
            " 11  | manhattan stories from the heart of a... | ['Manhattan (New ... |    4.16    | 0.000735\n",
            " 12  | the hobbit                               | ['Juvenile Fiction'] |    4.68    | 0.000727\n",
            " 13  | the hobbit there and back again          | ['Adventure stori... |    4.68    | 0.000727\n",
            " 14  | the hobbit or there and back again       | ['Juvenile Fiction'] |    4.68    | 0.000726\n",
            " 15  | the hobbitt, or there and back again;... | ['Fiction']          |    4.68    | 0.000724\n",
            " 16  | fahrenheit 451                           | ['Comics & Graphi... |    4.23    | 0.000715\n",
            " 17  | who moved my cheese? an-amazing way t... | ['Business & Econ... |    3.41    | 0.000708\n",
            " 18  | to kill a mocking bird                   | ['Drama']            |    4.60    | 0.000699\n",
            " 19  | harper lee's to kill a mockingbird       | ['Juvenile Nonfic... |    4.61    | 0.000699\n",
            " 20  | to kill a mockingbird                    | ['Performing Arts']  |    4.59    | 0.000699\n",
            "\n",
            "Top 20 book IDs from RDD PageRank: [65953, 22785, 147537, 55057, 80380, 104081, 156969, 64688, 176571, 63473, 92527, 158528, 158530, 158529, 158532, 52254, 193670, 183029, 65872, 183030]\n"
          ]
        }
      ],
      "source": [
        "# TOP 20 Books by RDD PageRank implementation\n",
        "rdd_iterations = iterations\n",
        "rdd_pagerank_top20 = ranks[:20]\n",
        "rdd_pagerank_ids = [book_id for book_id, score in rdd_pagerank_top20]\n",
        "\n",
        "# Create a formatted table with all required information\n",
        "print(f\"\\nTop 20 Books by RDD PageRank (converged in {rdd_iterations} iterations):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (book_id, pagerank_score) in enumerate(rdd_pagerank_top20):\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "    \n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "    \n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "    \n",
        "    # Print formatted row\n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")\n",
        "\n",
        "print(f\"\\nTop 20 book IDs from RDD PageRank: {rdd_pagerank_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## COMPARING THE RESULTS BETWEEN THREE LISTS <br>\n",
        "both in content (book titles) and their rankings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Common books in all three lists: 20/20\n",
            "Common books between Built-in and Python: 20/20\n",
            "Common books between Built-in and RDD: 20/20\n",
            "Common books between Python and RDD: 20/20\n",
            "\n",
            "Top 5 books comparison:\n",
            "Position 1: Built-in=65953, Python=65953, RDD=65953\n",
            "  Match: yes\n",
            "Position 2: Built-in=22785, Python=22785, RDD=22785\n",
            "  Match: yes\n",
            "Position 3: Built-in=147537, Python=147537, RDD=147537\n",
            "  Match: yes\n",
            "Position 4: Built-in=55057, Python=55057, RDD=55057\n",
            "  Match: yes\n",
            "Position 5: Built-in=80380, Python=80380, RDD=80380\n",
            "  Match: yes\n"
          ]
        }
      ],
      "source": [
        "# are they even similar?\n",
        "common_all = set(top_20_ids) & set(custom_pagerank_ids) & set(rdd_pagerank_ids)\n",
        "common_builtin_python = set(top_20_ids) & set(custom_pagerank_ids)\n",
        "common_builtin_rdd = set(top_20_ids) & set(rdd_pagerank_ids)\n",
        "common_python_rdd = set(custom_pagerank_ids) & set(rdd_pagerank_ids)\n",
        "\n",
        "# Print overlap statistics\n",
        "print(f\"Common books in all three lists: {len(common_all)}/20\")\n",
        "print(f\"Common books between Built-in and Python: {len(common_builtin_python)}/20\")\n",
        "print(f\"Common books between Built-in and RDD: {len(common_builtin_rdd)}/20\")\n",
        "print(f\"Common books between Python and RDD: {len(common_python_rdd)}/20\")\n",
        "\n",
        "# Check if the top books match across implementations\n",
        "print(\"\\nTop 5 books comparison:\")\n",
        "for i in range(5):\n",
        "    if i < len(top_20_ids) and i < len(custom_pagerank_ids) and i < len(rdd_pagerank_ids):\n",
        "        print(f\"Position {i+1}: Built-in={top_20_ids[i]}, Python={custom_pagerank_ids[i]}, RDD={rdd_pagerank_ids[i]}\")\n",
        "        match = (top_20_ids[i] == custom_pagerank_ids[i] == rdd_pagerank_ids[i])\n",
        "        print(f\"  Match: {'yes' if match else 'no'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only difference is 2,3 of positions. Neglegable since top 20 books are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **PHASE FIVE: TOPIC SENSITIVE PAGERANK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this one too we will be using pure python and rdd version.\n",
        "The only diffrence is that here we dont teleport to ANY place, Instead we travel to destinations that we are intrested in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pagerank_python_topic_sensitive(pages, links, genre_mapping, target_genres=None, \n",
        "                                   beta=0.85, max_iter=100, tol=1e-6, silent=False):\n",
        "\n",
        "    \n",
        "    # Input validation\n",
        "    if not pages:\n",
        "        raise ValueError(\"Pages list cannot be empty\")\n",
        "    if not links:\n",
        "        raise ValueError(\"Links list cannot be empty\")\n",
        "    if not isinstance(genre_mapping, dict):\n",
        "        raise ValueError(\"Genre mapping must be a dictionary\")\n",
        "    if not (0 <= beta <= 1):\n",
        "        raise ValueError(f\"Beta must be between 0 and 1, got {beta}\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(f\"Max iterations must be positive, got {max_iter}\")\n",
        "    if tol <= 0:\n",
        "        raise ValueError(f\"Tolerance must be positive, got {tol}\")\n",
        "    \n",
        "    N = len(pages)\n",
        "    if not silent:\n",
        "        print(f\"Starting Topic-Sensitive PageRank with {N:,} pages\")\n",
        "    \n",
        "    ranks = {p: 1.0 / N for p in pages}\n",
        "    initial_sum = sum(ranks.values())\n",
        "    if not silent:\n",
        "        print(f\"Initialized ranks (sum={initial_sum:.8f})\")\n",
        "\n",
        "    adjacency = defaultdict(list)\n",
        "    for src, dst in links:\n",
        "        adjacency[src].append(dst)\n",
        "    \n",
        "    nodes_with_outlinks = len(adjacency)\n",
        "    total_edges = len(links)\n",
        "    if not silent:\n",
        "        print(f\"Built adjacency list ({nodes_with_outlinks:,} nodes with outlinks, {total_edges:,} total edges)\")\n",
        "    \n",
        "    if target_genres is None:\n",
        "        # Original behavior: uniform teleportation to all pages\n",
        "        teleport_prob = (1 - beta) / N\n",
        "        teleport_probs = {p: teleport_prob for p in pages}\n",
        "        if not silent:\n",
        "            print(f\"Using uniform teleportation (original PageRank behavior)\")\n",
        "    else:\n",
        "        topic_pages = [p for p in pages if any(str(g) in str(genre_mapping.get(p, \"\")) for g in target_genres)]\n",
        "        \n",
        "        if not topic_pages:\n",
        "            # going back to uniform if no target pages found\n",
        "            teleport_prob = (1 - beta) / N\n",
        "            teleport_probs = {p: teleport_prob for p in pages}\n",
        "            if not silent:\n",
        "                print(f\"Warning: No pages found for target genres {target_genres}. Using uniform teleportation.\")\n",
        "        else:\n",
        "            topic_teleport_prob = (1 - beta) / len(topic_pages)\n",
        "            teleport_probs = {p: topic_teleport_prob if p in topic_pages else 0.0 for p in pages}\n",
        "            if not silent:\n",
        "                print(f\"Topic-sensitive teleportation: {len(topic_pages):,} pages in {target_genres}\")\n",
        "\n",
        "    if not silent:\n",
        "        print(f\"Starting PageRank iterations...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        new_ranks = {p: teleport_probs[p] for p in pages}\n",
        "\n",
        "        for src in pages:\n",
        "            neighbors = adjacency.get(src, [])\n",
        "            share = ranks[src] / len(neighbors)\n",
        "            for dst in neighbors:\n",
        "                new_ranks[dst] += beta * share\n",
        "\n",
        "        diff = (sum((new_ranks[p] - ranks[p])**2 for p in pages))**0.5  # L2 norm\n",
        "        rank_sum = sum(new_ranks.values())\n",
        "        \n",
        "        if not silent:\n",
        "            print(f\"Iteration {iteration + 1:3d}: L2 diff={diff:.6f}, sum={rank_sum:.8f}\")\n",
        "        \n",
        "        ranks = new_ranks\n",
        "        if diff < tol:\n",
        "            break\n",
        "\n",
        "    if not silent:\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"Converged after {iteration + 1} iterations\")\n",
        "        \n",
        "        # Final validation\n",
        "        final_sum = sum(ranks.values())\n",
        "        if abs(final_sum - 1.0) > 1e-6:\n",
        "            print(f\"Warning: Final sum ({final_sum:.8f}) deviates from 1.0\")\n",
        "        else:\n",
        "            print(f\"Rank sum validation passed\")\n",
        "\n",
        "    return sorted(ranks.items(), key=lambda x: -x[1]), iteration + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Topic-Sensitive PageRank with 47,488 pages\n",
            "Initialized ranks (sum=1.00000000)\n",
            "Built adjacency list (47,488 nodes with outlinks, 4,084,494 total edges)\n",
            "Topic-sensitive teleportation: 13,194 pages in ['Fiction']\n",
            "Starting PageRank iterations...\n",
            "------------------------------------------------------------\n",
            "Iteration   1: L2 diff=0.010422, sum=1.00000000\n",
            "Iteration   2: L2 diff=0.003976, sum=1.00000000\n",
            "Iteration   3: L2 diff=0.001157, sum=1.00000000\n",
            "Iteration   4: L2 diff=0.000737, sum=1.00000000\n",
            "Iteration   5: L2 diff=0.000490, sum=1.00000000\n",
            "Iteration   6: L2 diff=0.000361, sum=1.00000000\n",
            "Iteration   7: L2 diff=0.000267, sum=1.00000000\n",
            "Iteration   8: L2 diff=0.000213, sum=1.00000000\n",
            "Iteration   9: L2 diff=0.000165, sum=1.00000000\n",
            "Iteration  10: L2 diff=0.000137, sum=1.00000000\n",
            "Iteration  11: L2 diff=0.000109, sum=1.00000000\n",
            "Iteration  12: L2 diff=0.000093, sum=1.00000000\n",
            "Iteration  13: L2 diff=0.000075, sum=1.00000000\n",
            "Iteration  14: L2 diff=0.000065, sum=1.00000000\n",
            "Iteration  15: L2 diff=0.000053, sum=1.00000000\n",
            "Iteration  16: L2 diff=0.000046, sum=1.00000000\n",
            "Iteration  17: L2 diff=0.000037, sum=1.00000000\n",
            "Iteration  18: L2 diff=0.000033, sum=1.00000000\n",
            "Iteration  19: L2 diff=0.000027, sum=1.00000000\n",
            "Iteration  20: L2 diff=0.000023, sum=1.00000000\n",
            "Iteration  21: L2 diff=0.000019, sum=1.00000000\n",
            "Iteration  22: L2 diff=0.000017, sum=1.00000000\n",
            "Iteration  23: L2 diff=0.000014, sum=1.00000000\n",
            "Iteration  24: L2 diff=0.000012, sum=1.00000000\n",
            "Iteration  25: L2 diff=0.000010, sum=1.00000000\n",
            "Iteration  26: L2 diff=0.000009, sum=1.00000000\n",
            "Iteration  27: L2 diff=0.000007, sum=1.00000000\n",
            "Iteration  28: L2 diff=0.000006, sum=1.00000000\n",
            "Iteration  29: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  30: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  31: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  32: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  33: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  34: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  35: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  36: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  37: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  38: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  39: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  40: L2 diff=0.000001, sum=1.00000000\n",
            "------------------------------------------------------------\n",
            "Converged after 40 iterations\n",
            "Rank sum validation passed\n"
          ]
        }
      ],
      "source": [
        "# lets choose a genre\n",
        "result_two_fiction = pagerank_python_topic_sensitive(data_2['pages'], data_2['links'],genre_mapping, target_genres=['Fiction'], beta=0.85, max_iter=100, tol=1e-6, silent=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 20 Fiction Books (Topic-Sensitive PageRank, converged in 40 iterations):\n",
            "------------------------------------------------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1   | harry potter and the sorcerer's stone    | ['Juvenile Fiction'] |    4.69    | 0.001366\n",
            " 2   | five people you meet in heaven           | ['Fiction']          |    4.16    | 0.000905\n",
            " 3   | the catcher in the rye                   | ['Young Adult Fic... |    3.92    | 0.000893\n",
            " 4   | the hobbit                               | ['Juvenile Fiction'] |    4.68    | 0.000892\n",
            " 5   | the hobbit or there and back again       | ['Juvenile Fiction'] |    4.68    | 0.000891\n",
            " 6   | the hobbitt, or there and back again;... | ['Fiction']          |    4.68    | 0.000887\n",
            " 7   | the hobbit there and back again          | ['Adventure stori... |    4.68    | 0.000880\n",
            " 8   | prey                                     | ['Predation (Biol... |    3.50    | 0.000873\n",
            " 9   | the hobbit; or, there and back again     | Unknown              |    4.69    | 0.000827\n",
            " 10  | one for the money                        | ['Fiction']          |    4.39    | 0.000778\n",
            " 11  | harper lee's to kill a mockingbird       | ['Juvenile Nonfic... |    4.61    | 0.000774\n",
            " 12  | to kill a mockingbird                    | ['Performing Arts']  |    4.59    | 0.000774\n",
            " 13  | to kill a mocking bird                   | ['Drama']            |    4.60    | 0.000774\n",
            " 14  | the great gatsby                         | ['Fiction']          |    4.15    | 0.000766\n",
            " 15  | great gatsby                             | ['American Dream']   |    4.16    | 0.000746\n",
            " 16  | harry potter & the prisoner of azkaban   | ['Literary Critic... |    4.77    | 0.000744\n",
            " 17  | manhattan stories from the heart of a... | ['Manhattan (New ... |    4.16    | 0.000737\n",
            " 18  | shutter island                           | ['Fiction']          |    4.11    | 0.000728\n",
            " 19  | harry potter and the chamber of secrets  | ['Juvenile Fiction'] |    4.67    | 0.000724\n",
            " 20  | fahrenheit 451                           | ['Comics & Graphi... |    4.23    | 0.000722\n"
          ]
        }
      ],
      "source": [
        "# Extract the top 20 Fiction books from topic-sensitive PageRank\n",
        "fiction_pagerank_sorted, iterations = result_two_fiction\n",
        "fiction_pagerank_top20 = fiction_pagerank_sorted[:20]\n",
        "\n",
        "# Create a formatted table showing the Fiction-focused PageRank results\n",
        "print(f\"\\nTop 20 Fiction Books (Topic-Sensitive PageRank, converged in {iterations} iterations):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (book_id, pagerank_score) in enumerate(fiction_pagerank_top20):\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "    \n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "    \n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "    \n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pagerank_rdd_topic_sensitive(edges_rdd, nodes_count, genre_mapping, target_genres=None,\n",
        "                                damping_factor=0.85, max_iter=100, tolerance=1e-6, silent=False):\n",
        "\n",
        "    # Input validation \n",
        "    if not edges_rdd:\n",
        "        raise ValueError(\"Edges RDD cannot be empty\")\n",
        "    if nodes_count <= 0:\n",
        "        raise ValueError(f\"Nodes count must be positive, got {nodes_count}\")\n",
        "    if not isinstance(genre_mapping, dict):\n",
        "        raise ValueError(\"Genre mapping must be a dictionary\")\n",
        "    if not (0 <= damping_factor <= 1):\n",
        "        raise ValueError(f\"Damping factor must be between 0 and 1, got {damping_factor}\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(f\"Max iterations must be positive, got {max_iter}\")\n",
        "    if tolerance <= 0:\n",
        "        raise ValueError(f\"Tolerance must be positive, got {tolerance}\")\n",
        "    \n",
        "    if not silent:\n",
        "        print(f\"Starting Topic-Sensitive PageRank RDD with {nodes_count:,} nodes\")\n",
        "    \n",
        "    if not silent:\n",
        "        print(\"Building adjacency list...\")\n",
        "    adjacency_rdd = edges_rdd.groupByKey().mapValues(list).cache()\n",
        "    if not silent:\n",
        "        print(\"Adjacency list built and cached\")\n",
        "    \n",
        "    if not silent:\n",
        "        print(\"Identifying all nodes in the graph...\")\n",
        "    all_nodes = edges_rdd.flatMap(lambda x: [x[0], x[1]]).distinct().collect()\n",
        "    \n",
        "    if target_genres is None:\n",
        "        teleport_prob = (1 - damping_factor) / len(all_nodes)\n",
        "        teleport_probs = {node: teleport_prob for node in all_nodes}\n",
        "        if not silent:\n",
        "            print(f\"Using uniform teleportation (original PageRank behavior)\")\n",
        "            print(f\"Teleportation probability set: {teleport_prob:.8f}\")\n",
        "    else:\n",
        "        topic_nodes = [n for n in all_nodes if any(str(g) in str(genre_mapping.get(n, \"\")) for g in target_genres)]\n",
        "        \n",
        "        if not topic_nodes:\n",
        "            \n",
        "            teleport_prob = (1 - damping_factor) / len(all_nodes)\n",
        "            teleport_probs = {node: teleport_prob for node in all_nodes}\n",
        "            if not silent:\n",
        "                print(f\"Warning: No nodes found for target genres {target_genres}. Using uniform teleportation.\")\n",
        "                print(f\"Teleportation probability set: {teleport_prob:.8f}\")\n",
        "        else:\n",
        "            topic_teleport_prob = (1 - damping_factor) / len(topic_nodes)\n",
        "            teleport_probs = {n: topic_teleport_prob if n in topic_nodes else 0.0 for n in all_nodes}\n",
        "            if not silent:\n",
        "                print(f\"Topic-sensitive teleportation: {len(topic_nodes):,} nodes in {target_genres}\")\n",
        "                print(f\"Topic teleportation probability: {topic_teleport_prob:.8f}\")\n",
        "    \n",
        "    ranks = {node: 1.0 / len(all_nodes) for node in all_nodes}\n",
        "    initial_sum = sum(ranks.values())\n",
        "    if not silent:\n",
        "        print(f\"Initial ranks set (sum={initial_sum:.8f})\")\n",
        "        print(f\"Starting PageRank iterations...\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    for iteration in range(max_iter):\n",
        "        old_ranks = ranks.copy()\n",
        "        \n",
        "        ranks_bc = edges_rdd.context.broadcast(ranks)\n",
        "        \n",
        "        contributions = adjacency_rdd.flatMap(\n",
        "            lambda node_neighbors: [\n",
        "                (neighbor, damping_factor * ranks_bc.value[node_neighbors[0]] / len(node_neighbors[1]))\n",
        "                for neighbor in node_neighbors[1]\n",
        "            ]\n",
        "        ).reduceByKey(lambda a, b: a + b).collectAsMap()\n",
        "        \n",
        "        ranks = {node: teleport_probs[node] + contributions.get(node, 0) for node in all_nodes}\n",
        "        \n",
        "        diff = sum((ranks[node] - old_ranks[node])**2 for node in all_nodes)**0.5  # L2 norm\n",
        "        total_sum = sum(ranks.values())\n",
        "        \n",
        "        if not silent:\n",
        "            print(f\"Iteration {iteration + 1:3d}: L2 diff={diff:.6f}, sum={total_sum:.8f}\")\n",
        "        \n",
        "        ranks_bc.unpersist()\n",
        "        \n",
        "        if diff < tolerance:\n",
        "            if not silent:\n",
        "                print(\"-\" * 60)\n",
        "                print(f\"Converged after {iteration + 1} iterations\")\n",
        "            break\n",
        "    \n",
        "    if not silent:\n",
        "        if iteration + 1 == max_iter:\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"Reached maximum iterations ({max_iter}) without full convergence\")\n",
        "        \n",
        "        print(f\"Final rank sum: {sum(ranks.values()):.8f}\")\n",
        "        \n",
        "        # Final validation\n",
        "        final_sum = sum(ranks.values())\n",
        "        if abs(final_sum - 1.0) > 1e-6:\n",
        "            print(f\"Warning: Final sum ({final_sum:.8f}) deviates from 1.0\")\n",
        "        else:\n",
        "            print(f\"Rank sum validation passed\")\n",
        "    \n",
        "    adjacency_rdd.unpersist()\n",
        "    \n",
        "    # sorted ranks and the number of iterations performed\n",
        "    return sorted(ranks.items(), key=lambda x: -x[1]), (iteration + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Topic-Sensitive PageRank RDD with 47,488 nodes\n",
            "Building adjacency list...\n",
            "Adjacency list built and cached\n",
            "Identifying all nodes in the graph...\n",
            "Topic-sensitive teleportation: 13,194 nodes in ['Fiction']\n",
            "Topic teleportation probability: 0.00001137\n",
            "Initial ranks set (sum=1.00000000)\n",
            "Starting PageRank iterations...\n",
            "------------------------------------------------------------\n",
            "Iteration   1: L2 diff=0.010422, sum=1.00000000\n",
            "Iteration   2: L2 diff=0.003976, sum=1.00000000\n",
            "Iteration   3: L2 diff=0.001157, sum=1.00000000\n",
            "Iteration   4: L2 diff=0.000737, sum=1.00000000\n",
            "Iteration   5: L2 diff=0.000490, sum=1.00000000\n",
            "Iteration   6: L2 diff=0.000361, sum=1.00000000\n",
            "Iteration   7: L2 diff=0.000267, sum=1.00000000\n",
            "Iteration   8: L2 diff=0.000213, sum=1.00000000\n",
            "Iteration   9: L2 diff=0.000165, sum=1.00000000\n",
            "Iteration  10: L2 diff=0.000137, sum=1.00000000\n",
            "Iteration  11: L2 diff=0.000109, sum=1.00000000\n",
            "Iteration  12: L2 diff=0.000093, sum=1.00000000\n",
            "Iteration  13: L2 diff=0.000075, sum=1.00000000\n",
            "Iteration  14: L2 diff=0.000065, sum=1.00000000\n",
            "Iteration  15: L2 diff=0.000053, sum=1.00000000\n",
            "Iteration  16: L2 diff=0.000046, sum=1.00000000\n",
            "Iteration  17: L2 diff=0.000037, sum=1.00000000\n",
            "Iteration  18: L2 diff=0.000033, sum=1.00000000\n",
            "Iteration  19: L2 diff=0.000027, sum=1.00000000\n",
            "Iteration  20: L2 diff=0.000023, sum=1.00000000\n",
            "Iteration  21: L2 diff=0.000019, sum=1.00000000\n",
            "Iteration  22: L2 diff=0.000017, sum=1.00000000\n",
            "Iteration  23: L2 diff=0.000014, sum=1.00000000\n",
            "Iteration  24: L2 diff=0.000012, sum=1.00000000\n",
            "Iteration  25: L2 diff=0.000010, sum=1.00000000\n",
            "Iteration  26: L2 diff=0.000009, sum=1.00000000\n",
            "Iteration  27: L2 diff=0.000007, sum=1.00000000\n",
            "Iteration  28: L2 diff=0.000006, sum=1.00000000\n",
            "Iteration  29: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  30: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  31: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  32: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  33: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  34: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  35: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  36: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  37: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  38: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  39: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  40: L2 diff=0.000001, sum=1.00000000\n",
            "------------------------------------------------------------\n",
            "Converged after 40 iterations\n",
            "Final rank sum: 1.00000000\n",
            "Rank sum validation passed\n"
          ]
        }
      ],
      "source": [
        "ranks_topic_sensitive, iterations = pagerank_rdd_topic_sensitive(edges_rdd_fixed, nodes_count, genre_mapping, target_genres=[\"Fiction\"], damping_factor=0.85, max_iter=100, tolerance=1e-6, silent=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 20 Fiction Books (RDD Topic-Sensitive PageRank, converged in 40 iterations):\n",
            "------------------------------------------------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1   | harry potter and the sorcerer's stone    | ['Juvenile Fiction'] |    4.69    | 0.001366\n",
            " 2   | five people you meet in heaven           | ['Fiction']          |    4.16    | 0.000905\n",
            " 3   | the catcher in the rye                   | ['Young Adult Fic... |    3.92    | 0.000893\n",
            " 4   | the hobbit                               | ['Juvenile Fiction'] |    4.68    | 0.000892\n",
            " 5   | the hobbit or there and back again       | ['Juvenile Fiction'] |    4.68    | 0.000891\n",
            " 6   | the hobbitt, or there and back again;... | ['Fiction']          |    4.68    | 0.000887\n",
            " 7   | the hobbit there and back again          | ['Adventure stori... |    4.68    | 0.000880\n",
            " 8   | prey                                     | ['Predation (Biol... |    3.50    | 0.000873\n",
            " 9   | the hobbit; or, there and back again     | Unknown              |    4.69    | 0.000827\n",
            " 10  | one for the money                        | ['Fiction']          |    4.39    | 0.000778\n",
            " 11  | harper lee's to kill a mockingbird       | ['Juvenile Nonfic... |    4.61    | 0.000774\n",
            " 12  | to kill a mocking bird                   | ['Drama']            |    4.60    | 0.000774\n",
            " 13  | to kill a mockingbird                    | ['Performing Arts']  |    4.59    | 0.000774\n",
            " 14  | the great gatsby                         | ['Fiction']          |    4.15    | 0.000766\n",
            " 15  | great gatsby                             | ['American Dream']   |    4.16    | 0.000746\n",
            " 16  | harry potter & the prisoner of azkaban   | ['Literary Critic... |    4.77    | 0.000744\n",
            " 17  | manhattan stories from the heart of a... | ['Manhattan (New ... |    4.16    | 0.000737\n",
            " 18  | shutter island                           | ['Fiction']          |    4.11    | 0.000728\n",
            " 19  | harry potter and the chamber of secrets  | ['Juvenile Fiction'] |    4.67    | 0.000724\n",
            " 20  | fahrenheit 451                           | ['Comics & Graphi... |    4.23    | 0.000722\n"
          ]
        }
      ],
      "source": [
        "# Extract top 20 Fiction books from the RDD-based topic-sensitive PageRank\n",
        "rdd_fiction_top20 = ranks_topic_sensitive[:20]\n",
        "\n",
        "# Create a formatted table showing the Fiction-focused PageRank results (RDD version)\n",
        "print(f\"\\nTop 20 Fiction Books (RDD Topic-Sensitive PageRank, converged in {iterations} iterations):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (book_id, pagerank_score) in enumerate(rdd_fiction_top20):\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "    \n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "    \n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "    \n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the above functions for the classic pagerank too: <br>\n",
        "\n",
        "classic_ranks, iterations = pagerank_rdd_topic_sensitive(<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    edges_rdd_fixed, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    nodes_count, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    genre_mapping, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    target_genres=None,  # This makes it behave like classic PageRank<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    damping_factor=0.85, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    max_iter=100, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    tolerance=1e-6, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    silent=False<br>\n",
        ")<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the above functions for the classic pagerank too: <br>\n",
        "\n",
        "classic_ranks, iterations = pagerank_python_topic_sensitive(<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;data_2['pages'], <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;data_2['links'],<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;genre_mapping, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;target_genres=None,  # This makes it behave like classic PageRank<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;beta=0.85, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;max_iter=100, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;tol=1e-6, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;silent=False<br>\n",
        ")<br>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

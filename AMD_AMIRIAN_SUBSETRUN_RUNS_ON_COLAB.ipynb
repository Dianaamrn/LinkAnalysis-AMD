{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PAGERANK FOR AMAZON BOOK REVIEWS\n",
        "## AMD - University of Milan\n",
        "### Fatemeh Amirian 34015A"
      ],
      "metadata": {
        "id": "Ycwhce5T5JVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output was taken on a 20% subset. Change if you wish to see different results on different fractions of the data."
      ],
      "metadata": {
        "id": "3YNEPAIs5hwC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCl0T41QMEz1"
      },
      "source": [
        "# PHASE ZERO: SET UP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_d05AmVNc7z",
        "outputId": "94965df4-0f92-42c0-be37-019629be1c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2025-06-11 21:39:43--  https://dlcdn.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400923510 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.6-bin-hadoop3.tgz.1’\n",
            "\n",
            "spark-3.5.6-bin-had 100%[===================>] 382.35M   290MB/s    in 1.3s    \n",
            "\n",
            "2025-06-11 21:39:58 (290 MB/s) - ‘spark-3.5.6-bin-hadoop3.tgz.1’ saved [400923510/400923510]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install openjdk-11-jdk-headless -qq\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.6-bin-hadoop3.tgz\n",
        "!pip install -q findspark pyspark==3.5.6 graphframes kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import findspark\n",
        "import matplotlib.pyplot as plt\n",
        "from graphframes import GraphFrame\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import (\n",
        "    DoubleType,\n",
        "    LongType,\n",
        "    StringType,\n",
        "    StructField,\n",
        "    StructType\n",
        ")\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import BooleanType\n",
        "import re"
      ],
      "metadata": {
        "id": "VNmAo47nPNAo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.6-bin-hadoop3\"\n",
        "\n",
        "try:\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    sc.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "# Setup Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PageRankForAmazonBookReviews\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"6g\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.4-spark3.5-s_2.12\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Test GraphFrames import\n",
        "from graphframes import GraphFrame\n",
        "print(\"GraphFrames imported successfully!\")"
      ],
      "metadata": {
        "id": "D9YbANSnN4f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beff6783-163e-4116-da2f-96537df45da6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphFrames imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "-8t29QfoNc3o",
        "outputId": "a0d4ff3f-ace7-4f21-b9d1-a4313da4b3de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7af3c888ae90>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7e0b01280215:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.6</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PageRankForAmazonBookReviews</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TBqHUCkOdyD"
      },
      "source": [
        "Downloading the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wcX1qyZZOfvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4da4c6-c945-4a31-82c9-535f1e8b46bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "amazon-books-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "# Replace \"xxxxxx\" with your actual Kaggle username and API key\n",
        "os.environ['KAGGLE_USERNAME'] = \"xxx\"\n",
        "os.environ['KAGGLE_KEY'] = \"xxx\"\n",
        "\n",
        "# Download Amazon Books Review dataset\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N88NFEu5O9JP"
      },
      "outputs": [],
      "source": [
        "# prompt: unzip the data\n",
        "\n",
        "with zipfile.ZipFile('amazon-books-reviews.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JQghO1dMEz8"
      },
      "source": [
        "# PHASE ONE : DATA PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vYOFhfIlPCM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef10d46-d0fb-45b1-9741-7b4920c66022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- review/score: double (nullable = true)\n",
            "\n",
            "+----------+-----------------------------+--------------+------------+\n",
            "|Id        |Title                        |User_id       |review/score|\n",
            "+----------+-----------------------------+--------------+------------+\n",
            "|0826414346|Dr. Seuss: American Icon     |A2RSSXTDZDUSH4|5.0         |\n",
            "|0595344550|Whispers of the Wicked Saints|AUR0VA5H0C66C |1.0         |\n",
            "|0595344550|Whispers of the Wicked Saints|ACO23CG8K8T77 |5.0         |\n",
            "|0595344550|Whispers of the Wicked Saints|AJV5HX8BBZKEP |4.0         |\n",
            "|0595344550|Whispers of the Wicked Saints|A2XXVRH6VJ8S7Q|5.0         |\n",
            "+----------+-----------------------------+--------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# reading the dataset and selecting the required columns\n",
        "selected_book_reviews = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"quote\", '\"') \\\n",
        "    .option(\"escape\", '\"') \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .csv(\"/content/Books_rating.csv\") \\\n",
        "    .select(\"Id\", \"Title\", \"User_id\", \"review/score\")\n",
        "\n",
        "# taking a sample since colab is limited\n",
        "selected_book_reviews = selected_book_reviews.sample(0.2, seed=42)\n",
        "\n",
        "\n",
        "# getting a glimpse of the dataset\n",
        "selected_book_reviews.printSchema()\n",
        "selected_book_reviews.cache()\n",
        "selected_book_reviews.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ghbg-D6B4wTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f54297-9e40-4dd7-88eb-02cc70f020e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of reviews: 600871\n"
          ]
        }
      ],
      "source": [
        "# how many rows do we have in the dataset?\n",
        "print(f\"Total number of reviews: {selected_book_reviews.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fr_07bsU494E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6d257d-aa53-43e7-bdd5-550cdbb5fd06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+------------+\n",
            "| Id|Title|User_id|review/score|\n",
            "+---+-----+-------+------------+\n",
            "|  0|   41| 112696|           0|\n",
            "+---+-----+-------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do we have any missing values in the columns we selected?\n",
        "selected_book_reviews.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in selected_book_reviews.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aSIurjfZ58xV"
      },
      "outputs": [],
      "source": [
        "# we drop the rows with missing User_id values\n",
        "selected_book_reviews = selected_book_reviews.na.drop(subset=[\"User_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2oNyknek6JI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d81a452-4051-4768-81f7-0ad8e7c3b2cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+------------+\n",
            "| Id|Title|User_id|review/score|\n",
            "+---+-----+-------+------------+\n",
            "|  0|    0|      0|           0|\n",
            "+---+-----+-------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# we need to fill the missing values in the Title column with a placeholder - unknown\n",
        "selected_book_reviews = selected_book_reviews.fillna(\"unknown\", subset=[\"Title\"])\n",
        "\n",
        "# let's check again for missing values\n",
        "selected_book_reviews.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in selected_book_reviews.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "M_os5quZ6mm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bf2c97-4f7e-48ca-daa6-f4eaca623b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate reviews (User_id, Book_id):\n",
            "+--------------+----------+-----+\n",
            "|       User_id|        Id|count|\n",
            "+--------------+----------+-----+\n",
            "|A2PBKFCD7YI23H|B000GY0PV4|    2|\n",
            "|A1L43KWWR05PCS|B000NWXNIG|    2|\n",
            "|A2CA0MYM4FCQSJ|0195813618|    2|\n",
            "|A2SPUG1DO8TH3R|0897501446|    2|\n",
            "|A3V1EPSE6XDC0C|B00005NKL9|    2|\n",
            "+--------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Number of user-book pairs with duplicate reviews: 1788\n"
          ]
        }
      ],
      "source": [
        "# we wanna check to see if we have any duplicate reviews by the same user for the same book Id?\n",
        "duplicate_reviews = selected_book_reviews.groupBy(\"User_id\", \"Id\").count().filter(\"count > 1\")\n",
        "\n",
        "# seeing the first few duplicate reviews\n",
        "print(\"Duplicate reviews (User_id, Book_id):\")\n",
        "duplicate_reviews.show(5)\n",
        "\n",
        "# counting the number of duplicate reviews (user-book id pairs)\n",
        "num_duplicate_reviews = duplicate_reviews.count()\n",
        "print(f\"Number of user-book pairs with duplicate reviews: {num_duplicate_reviews}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RJ_-hFqc7Ley",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563b1412-393b-45a8-fdcf-855dbc6fff9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews after removing duplicates: 486012\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Id: string, Title: string, User_id: string, review/score: double]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# removing duplicates based on User_id and Id\n",
        "cleaned_reviews = selected_book_reviews.dropDuplicates([\"User_id\", \"Id\"])\n",
        "print(f\"Number of reviews after removing duplicates: {cleaned_reviews.count()}\")\n",
        "\n",
        "cleaned_reviews = cleaned_reviews.cache()\n",
        "selected_book_reviews.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IqUebRuORAm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3cb5ae1-770e-4e3b-a538-55350583d672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique books: 108,488\n",
            "Unique users: 305,881\n",
            "+----------+--------------------+-----+\n",
            "|        Id|               Title|count|\n",
            "+----------+--------------------+-----+\n",
            "|B000IEZE3G|Harry Potter and ...|  794|\n",
            "|B000Q032UY|The Hobbit or The...|  736|\n",
            "|B000GQG5MA|The Hobbit; Or, T...|  735|\n",
            "|B000GQG7D2|          The Hobbit|  719|\n",
            "|B000PC54NG|          The Hobbit|  717|\n",
            "+----------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# checking to see how many unique books and user Ids we have in the dataset\n",
        "unique_books = cleaned_reviews.select(\"Id\").distinct().count()\n",
        "unique_users = cleaned_reviews.select(\"User_id\").distinct().count()\n",
        "\n",
        "print(f\"Unique books: {unique_books:,}\")\n",
        "print(f\"Unique users: {unique_users:,}\")\n",
        "\n",
        "\n",
        "# See books with most reviews - we will likely see some of these books in the top lists\n",
        "cleaned_reviews.groupBy(\"Id\", \"Title\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h324tWSaMEz9"
      },
      "source": [
        "### **Attention!!!** Here we notice something important. We seem to have similar books (same titles) but with different book ids. we need to test this hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFMwEd9FMEz9"
      },
      "source": [
        "First we normalize the titles (at least as much as we can). <br>\n",
        "1. Make all of them lower case\n",
        "2. Remove versions such as (CD) - (Audio Book) **but we will exculde generic titles such as poems because in that case we would lose information**\n",
        "<br>\n",
        "\n",
        "The idea is that I want to see the true books by their content and not by their version. If i am ranking the books by their importance, I dont care if it was paper back or hard cover and people do tend to leave diffrent reviwes for diferent editions of the same book. <br>\n",
        "However, Perfect normalization will not be achieved in this attempt but it will make our ranking more meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oue3JzBzMEz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c642ef0-9ac3-4587-b355-ec81eb8ff38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique original titles: 104,134\n",
            "Unique normalized titles: 100,522\n",
            "Reduction: 3,612 (3.47%)\n",
            "\n",
            "Sample data with normalized titles:\n",
            "+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+\n",
            "|Title                                                                               |Title_Norm                                                                          |\n",
            "+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+\n",
            "|Books Into Film: The Stuff That Dreams Are Made of                                  |books into film: the stuff that dreams are made of                                  |\n",
            "|Political Ideals                                                                    |political ideals                                                                    |\n",
            "|Monsieur Beaucaire                                                                  |monsieur beaucaire                                                                  |\n",
            "|Making I/T Work: An Executive's Guide to Implementing Information Technology Systems|making i/t work: an executive's guide to implementing information technology systems|\n",
            "|The Last Two Million Years                                                          |the last two million years                                                          |\n",
            "|Norman Mailer: The Radical as Hipster                                               |norman mailer: the radical as hipster                                               |\n",
            "|Hooked on Fish: All the Fish You Can Crochet                                        |hooked on fish: all the fish you can crochet                                        |\n",
            "|The Working Director: How to Arrive, Survive and Thrive in the Director's Chair     |the working director: how to arrive, survive and thrive in the director's chair     |\n",
            "|Horror Show                                                                         |horror show                                                                         |\n",
            "|Autodesk 3ds Max 8 Revealed                                                         |autodesk 3ds max 8 revealed                                                         |\n",
            "+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# normalizing the titles\n",
        "# define a list of generic titles that we want to keep as is - i took a wild guess with this based on my experince in reading books\n",
        "# also we make it into a function because we will need it for mappings\n",
        "\n",
        "def normalize_titles(df, title_column=\"Title\"):\n",
        "\n",
        "    # Define generic titles to preserve as-is\n",
        "    generic_titles = [\n",
        "        \"poems\", \"selected poems\", \"collected poems\",\n",
        "        \"essays\", \"letters\", \"stories\", \"collected stories\",\n",
        "        \"short stories\", \"anthology\", \"complete works\",\n",
        "        \"selected works\", \"collected works\", \"memoir\",\n",
        "        \"biography\", \"autobiography\", \"diary\", \"journals\"\n",
        "    ]\n",
        "\n",
        "    @F.udf(returnType=BooleanType())\n",
        "    def contains_generic_title(title):\n",
        "        if title is None:\n",
        "            return False\n",
        "        for generic in generic_titles:\n",
        "            if re.search(r'\\b' + re.escape(generic) + r'\\b', title):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    df_with_lowercase = df.withColumn(\n",
        "        \"LowercasedTitle\",\n",
        "        F.lower(F.col(title_column))\n",
        "    )\n",
        "\n",
        "    df_normalized = df_with_lowercase.withColumn(\n",
        "        \"Title_Norm\",\n",
        "        F.when(\n",
        "            contains_generic_title(F.col(\"LowercasedTitle\")),\n",
        "            F.col(\"LowercasedTitle\")\n",
        "        ).otherwise(\n",
        "            F.trim(F.regexp_replace(\n",
        "                F.col(\"LowercasedTitle\"),\n",
        "                r\"\\[.*?\\]|\\(.*?\\)|audiobook|unabridged|cd\",\n",
        "                \"\"\n",
        "            ))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    df_normalized = df_normalized.drop(\"LowercasedTitle\")\n",
        "\n",
        "    return df_normalized\n",
        "\n",
        "\n",
        "cleaned_reviews_norm = normalize_titles(cleaned_reviews)\n",
        "\n",
        "cleaned_reviews_norm = cleaned_reviews_norm.cache()\n",
        "cleaned_reviews.unpersist()\n",
        "\n",
        "orig_count = cleaned_reviews_norm.select(\"Title\").distinct().count()\n",
        "norm_count = cleaned_reviews_norm.select(\"Title_Norm\").distinct().count()\n",
        "\n",
        "print(f\"Unique original titles: {orig_count:,}\")\n",
        "print(f\"Unique normalized titles: {norm_count:,}\")\n",
        "\n",
        "# what percent of titles were reduced?\n",
        "print(f\"Reduction: {orig_count - norm_count:,} ({(orig_count - norm_count)/orig_count*100:.2f}%)\")\n",
        "\n",
        "# show some examples to see if we did a good job\n",
        "print(\"\\nSample data with normalized titles:\")\n",
        "cleaned_reviews_norm.select(\"Title\", \"Title_Norm\").distinct().orderBy(F.rand()).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95WNAOnGMEz-"
      },
      "source": [
        "Since This looks somewhat okay, we abondon the told title colomns and continue with our new colomn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ATUbbtSoMEz-"
      },
      "outputs": [],
      "source": [
        "# Rename NormalizedTitle to Title_Norm\n",
        "cleaned_reviews_norm = cleaned_reviews_norm.withColumnRenamed(\"NormalizedTitle\", \"Title_Norm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_XhLwh1cMEz-"
      },
      "outputs": [],
      "source": [
        "cleaned_reviews_norm = cleaned_reviews_norm.drop(\"Title\")\n",
        "cleaned_reviews_norm = cleaned_reviews_norm.drop(\"LowercasedTitle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CnTXr7MXMEz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858a2d9e-f279-448d-ffba-55e6093b3a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema after replacing Title with normalized version:\n",
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- review/score: double (nullable = true)\n",
            " |-- Title_Norm: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify the change\n",
        "print(\"Schema after replacing Title with normalized version:\")\n",
        "cleaned_reviews_norm.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xX1FOJPnMEz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70dd4df-c79e-447c-b9bc-5f689bd32a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+------------+-----------------------------------------------------+\n",
            "|Id        |User_id       |review/score|Title_Norm                                           |\n",
            "+----------+--------------+------------+-----------------------------------------------------+\n",
            "|B0007JGWB0|A100Z0BS6Z89IN|5.0         |seven pillars of wisdom,: a triumph                  |\n",
            "|B0000CJ9GZ|A102DWIIFU8MWF|4.0         |the richest man in babylon                           |\n",
            "|B000G643YM|A1042BIXF6ZMAC|5.0         |little women                                         |\n",
            "|B000Q032UY|A104QFFOEJL0NW|5.0         |the hobbit or there and back again                   |\n",
            "|0884196372|A106EO2I13BL2L|5.0         |image maker, the: recognize your true worth and value|\n",
            "+----------+--------------+------------+-----------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_reviews_norm.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rweB4s_OMEz-"
      },
      "source": [
        "#### Now the next problem is that titles sometimes have different book IDs (despite being the same book) and we need to verify this before thinking of a solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t1T-Ni5T8tRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3f146e-20fe-4cf4-9764-9057a910e78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of titles with the same title but different IDs:\n",
            "5442\n",
            "\n",
            "Top five books with the same title and different IDs ordered by the count of distinct ids:\n",
            "+--------------------------------------------------------------+-----------------+\n",
            "|Title_Norm                                                    |distinct_id_count|\n",
            "+--------------------------------------------------------------+-----------------+\n",
            "|persuasion                                                    |18               |\n",
            "|wuthering heights                                             |17               |\n",
            "|great expectations                                            |15               |\n",
            "|emma                                                          |14               |\n",
            "|the picture of dorian gray                                    |13               |\n",
            "|an inquiry into the nature and causes of the wealth of nations|12               |\n",
            "|the white company                                             |12               |\n",
            "|pride and prejudice                                           |11               |\n",
            "|little men                                                    |11               |\n",
            "|romola                                                        |11               |\n",
            "+--------------------------------------------------------------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# grouping books by Title and count how many distinct IDs exist for each title\n",
        "title_id_counts = cleaned_reviews_norm.groupBy(\"Title_Norm\").agg(F.countDistinct(\"Id\").alias(\"distinct_id_count\"))\n",
        "\n",
        "# Filter titles with more than one distinct ID\n",
        "titles_with_same_name_diff_id = title_id_counts.filter(col(\"distinct_id_count\") > 1)\n",
        "\n",
        "print(\"Number of titles with the same title but different IDs:\")\n",
        "duplicate_titles = titles_with_same_name_diff_id.count()\n",
        "print(duplicate_titles)\n",
        "\n",
        "# top ten books with the most distinct IDs - same title\n",
        "print(\"\\nTop five books with the same title and different IDs ordered by the count of distinct ids:\")\n",
        "titles_with_same_name_diff_id.orderBy(col(\"distinct_id_count\").desc()).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTHc3UFOMEz_"
      },
      "source": [
        "For the purpose of truly having distincs Human - Book reviews and also easier handling in the functions, we will create integer indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QADA_UGFMEz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd94076e-eda9-493b-e682-8f5a133bdd67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------+--------------------+-------+-----+\n",
            "|       User_id|user_id_int|        Id|          Title_Norm|book_id|score|\n",
            "+--------------+-----------+----------+--------------------+-------+-----+\n",
            "|A1005YJDO9VCIY|        124|0977390403|the gift of the a...|  78415|  5.0|\n",
            "|A1006V961PBMKA|        127|0742516814|fighting the forc...|  26873|  3.0|\n",
            "|A100HWDN5JMK8G|        139|1413703860|         jake's gold|  39401|  5.0|\n",
            "|A1018G2FPJBJ0S|        190|B000KW0HGK|essays of ralph w...|  24925|  4.0|\n",
            "|A10AKE9TAADHVV|        769|B0006ASR90|insects: a guide ...|  37991|  4.0|\n",
            "|A10AKE9TAADHVV|        769|0679886524|the berenstain be...|  72425|  5.0|\n",
            "|A10AKE9TAADHVV|        769|B0006BQLVA|pond life - a gui...|  57329|  4.0|\n",
            "|A10AKE9TAADHVV|        769|1559716843|     fun with nature|  29306|  5.0|\n",
            "|A10AZVYK32ZJSE|        803|B000871DZ6|alice's adventure...|   5163|  5.0|\n",
            "|A10AZVYK32ZJSE|        803|1596009772|alice's adventure...|   5159|  5.0|\n",
            "+--------------+-----------+----------+--------------------+-------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "def add_generated_ids_norm(df):\n",
        "\n",
        "    title_window = Window.orderBy(\"Title_Norm\")\n",
        "    titles_with_id = df.select(\"Title_Norm\").distinct() \\\n",
        "                      .withColumn(\"book_id\", F.dense_rank().over(title_window))\n",
        "\n",
        "    user_window = Window.orderBy(\"User_id\")\n",
        "    users_with_id = df.select(\"User_id\").distinct() \\\n",
        "                     .withColumn(\"user_id_int\", F.dense_rank().over(user_window))\n",
        "\n",
        "    result = df.join(titles_with_id, \"Title_Norm\") \\\n",
        "               .join(users_with_id, \"User_id\") \\\n",
        "               .select(\"User_id\", \"user_id_int\", \"Id\", \"Title_Norm\", \"book_id\",\n",
        "                       F.col(\"review/score\").cast(\"double\").alias(\"score\"))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "df_with_ids = add_generated_ids_norm(cleaned_reviews_norm)\n",
        "df_with_ids = df_with_ids.cache()\n",
        "cleaned_reviews_norm.unpersist()\n",
        "df_with_ids.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MksXf-TeMEz_"
      },
      "source": [
        "Checking to see if there is any collision?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tIiqVxD4MEz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac34564d-a3db-4cf4-afff-76b1b5255893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book collisions: 0\n",
            "User collisions: 0\n"
          ]
        }
      ],
      "source": [
        "book_collisions = df_with_ids.select(\"book_id\", \"Title_Norm\").distinct().groupBy(\"book_id\").count().filter(F.col(\"count\") > 1).count()\n",
        "user_collisions = df_with_ids.select(\"user_id_int\", \"User_id\").distinct().groupBy(\"user_id_int\").count().filter(F.col(\"count\") > 1).count()\n",
        "\n",
        "print(f\"Book collisions: {book_collisions}\")\n",
        "print(f\"User collisions: {user_collisions}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vXKC8EySMEz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880c3c64-5f76-4d7a-ca50-64f57c7a03ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+--------------------+-----+\n",
            "|user_id_int|book_id|          Title_Norm|score|\n",
            "+-----------+-------+--------------------+-----+\n",
            "|        124|  78415|the gift of the a...|  5.0|\n",
            "|        127|  26873|fighting the forc...|  3.0|\n",
            "|        139|  39401|         jake's gold|  5.0|\n",
            "|        190|  24925|essays of ralph w...|  4.0|\n",
            "|        769|  37991|insects: a guide ...|  4.0|\n",
            "|        769|  72425|the berenstain be...|  5.0|\n",
            "|        769|  57329|pond life - a gui...|  4.0|\n",
            "|        769|  29306|     fun with nature|  5.0|\n",
            "|        803|   5163|alice's adventure...|  5.0|\n",
            "|        803|   5159|alice's adventure...|  5.0|\n",
            "+-----------+-------+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# only keeping what we need\n",
        "indexed_data = df_with_ids.select(\"user_id_int\", \"book_id\", \"Title_Norm\", \"score\")\n",
        "indexed_data.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY3hWDPvME0A"
      },
      "source": [
        "Here we can see that we run into more duplicate human-book pairs which makes sense since we basically turned many different book variations into one id, so we remove duplicates again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8zuxsrsJME0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26315cb-7209-46b6-85d8-ccbf7811765f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original count: 486012\n",
            "After removing duplicates: 450647\n",
            "Duplicates removed: 35365\n"
          ]
        }
      ],
      "source": [
        "# removng duplicate user-book pairs\n",
        "indexed_data_unique = indexed_data.dropDuplicates([\"user_id_int\", \"book_id\"])\n",
        "\n",
        "indexed_data_unique = indexed_data_unique.cache()\n",
        "indexed_data.unpersist()\n",
        "df_with_ids.unpersist()\n",
        "\n",
        "a = indexed_data.count()\n",
        "b = indexed_data_unique.count()\n",
        "\n",
        "print(\"Original count:\", a)\n",
        "print(\"After removing duplicates:\", b)\n",
        "print(\"Duplicates removed:\", a-b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_-hnL0F9ME0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959dbace-1cdb-47e5-a03b-a2f6fa5ffa92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+--------------------+-----+\n",
            "|user_id_int|book_id|          Title_Norm|score|\n",
            "+-----------+-------+--------------------+-----+\n",
            "|         63|  32121|gulliver's travel...|  5.0|\n",
            "|        479|   6854|anglo-saxon attit...|  3.0|\n",
            "|        492|  14943|chef prudhomme's ...|  5.0|\n",
            "|        668|  56569|pigeon feathers &...|  5.0|\n",
            "|        768|   5398|all the weyrs of ...|  5.0|\n",
            "+-----------+-------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexed_data_unique.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dBGmvnpvME0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5d3072-dd08-4893-d265-43f4a3827527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique books: 100,522\n",
            "Number of unique users: 305,881\n",
            "Average reviews per user: 1.47\n",
            "+-----------+---+---+---+-----------+\n",
            "|min_reviews| Q1| Q2| Q3|max_reviews|\n",
            "+-----------+---+---+---+-----------+\n",
            "|          1|1.0|1.0|1.0|       1127|\n",
            "+-----------+---+---+---+-----------+\n",
            "\n",
            "\n",
            "Top 10 Most Active Reviewers:\n",
            "+-----------+-----+\n",
            "|user_id_int|count|\n",
            "+-----------+-----+\n",
            "|      10567| 1127|\n",
            "|     260617|  694|\n",
            "|     263952|  369|\n",
            "|      74959|  302|\n",
            "|      45290|  276|\n",
            "|      63429|  242|\n",
            "|      29398|  224|\n",
            "|     135961|  205|\n",
            "|      52011|  168|\n",
            "|      82014|  161|\n",
            "+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Count unique books\n",
        "unique_books = indexed_data_unique.select(\"book_id\").distinct().count()\n",
        "\n",
        "# Count unique users\n",
        "unique_users = indexed_data_unique.select(\"user_id_int\").distinct().count()\n",
        "\n",
        "# Calculate average reviews per user\n",
        "reviews_per_user = indexed_data_unique.groupBy(\"user_id_int\").count()\n",
        "avg_reviews_per_user = reviews_per_user.select(F.avg(\"count\")).first()[0]\n",
        "\n",
        "# Print the statistics\n",
        "print(f\"Number of unique books: {unique_books:,}\")\n",
        "print(f\"Number of unique users: {unique_users:,}\")\n",
        "print(f\"Average reviews per user: {avg_reviews_per_user:.2f}\")\n",
        "\n",
        "# Additional distribution statistics\n",
        "reviews_distribution = reviews_per_user.select(\n",
        "    F.min(\"count\").alias(\"min_reviews\"),\n",
        "    F.expr(\"percentile(count, 0.25)\").alias(\"Q1\"),\n",
        "    F.expr(\"percentile(count, 0.5)\").alias(\"Q2\"),\n",
        "    F.expr(\"percentile(count, 0.75)\").alias(\"Q3\"),\n",
        "    F.max(\"count\").alias(\"max_reviews\")\n",
        ")\n",
        "\n",
        "reviews_distribution.show()\n",
        "\n",
        "# Show top 10 most active reviewers\n",
        "print(\"\\nTop 10 Most Active Reviewers:\")\n",
        "reviews_per_user.orderBy(F.col(\"count\").desc()).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So most people only leave one review."
      ],
      "metadata": {
        "id": "e_TYX5el6ifK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Mc2l_KME0A"
      },
      "source": [
        "### **ATTENTION** <br>\n",
        "Because during debugging the spark session would often crash and i had to restart the kernel, i saved the final data so each time that i would re-run the spark, i wouldnt have to re-run the data processing too. <br>\n",
        "be careful this snippet of code will make and save inside the relative directory. You might wanna skip the next 2 blocks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = indexed_data_unique"
      ],
      "metadata": {
        "id": "SVDa4qMoUEr5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "b1AvqgiHME0B"
      },
      "outputs": [],
      "source": [
        "# Convert to pandas DataFrame for reliable local storage\n",
        "#pandas_df = final_data.toPandas()\n",
        "#os.makedirs(\"pandas_data\", exist_ok=True)\n",
        "#pandas_df.to_csv(\"pandas_data/final_data.csv\", index=False)\n",
        "#pandas_df.to_pickle(\"pandas_data/final_data.pkl\")\n",
        "\n",
        "#print(f\"Successfully saved DataFrame with {len(pandas_df)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BWcwlO4ME0B"
      },
      "source": [
        "ONLY USE IF THE SPARK CRASHES AND YOU ARE TRYING TO RESTART THE SESSION."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "w8eHQqBUME0B"
      },
      "outputs": [],
      "source": [
        "#final_data = spark.read \\.option(\"header\", \"true\") \\.option(\"inferSchema\", \"true\") \\.csv(\"pandas_data/final_data.csv\")\n",
        "#final_data.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71xw9j1OME0B"
      },
      "source": [
        "## PHASE ONE (1.1): MAPPINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2JbRuzy8ME0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded6454b-bd59-4afa-8db1-a9309324d7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created mapping for 100522 unique books\n",
            "\n",
            "Sample title mappings:\n",
            "Book ID: 40576 ---> Title: just another kid\n",
            "Book ID: 12034 ---> Title: book of fresh flowers: a complete guide to selecting and arranging\n",
            "Book ID: 59360 ---> Title: quick-strip paper piecing\n",
            "Book ID: 63534 ---> Title: second foundation\n"
          ]
        }
      ],
      "source": [
        "# title - book id\n",
        "def create_title_mapping(df):\n",
        "    title_rows = df.select(\"book_id\", \"Title_Norm\").distinct().collect()\n",
        "\n",
        "    title_mapping = {row[\"book_id\"]: row[\"Title_Norm\"] for row in title_rows}\n",
        "\n",
        "    print(f\"Created mapping for {len(title_mapping)} unique books\")\n",
        "    return title_mapping\n",
        "\n",
        "title_mapping = create_title_mapping(final_data)\n",
        "\n",
        "# Preview a few entries\n",
        "print(\"\\nSample title mappings:\")\n",
        "for book_id, title in list(title_mapping.items())[:4]:\n",
        "    print(f\"Book ID: {book_id} ---> Title: {title}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TSwbRgQpME0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223eebfa-8e04-4ec6-e7ef-db8a633ce09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created rating mapping for 100522 books\n",
            "\n",
            "Sample book rating mappings:\n",
            "Book ID: 6654 ---> Rating: 5.00/5 (1 reviews)\n",
            "Book ID: 44906 ---> Rating: 3.78/5 (94 reviews)\n",
            "Book ID: 65867 ---> Rating: 5.00/5 (6 reviews)\n",
            "Book ID: 34239 ---> Rating: 5.00/5 (3 reviews)\n",
            "Book ID: 89537 ---> Rating: 4.67/5 (3 reviews)\n"
          ]
        }
      ],
      "source": [
        "# average rating - book id\n",
        "def create_book_rating_mapping(df):\n",
        "    avg_ratings = df.groupBy(\"book_id\") \\\n",
        "                    .agg(F.avg(\"score\").alias(\"avg_rating\"),\n",
        "                         F.count(\"score\").alias(\"num_ratings\")) \\\n",
        "                    .collect()\n",
        "\n",
        "    rating_mapping = {row[\"book_id\"]: {\n",
        "                          \"avg_rating\": float(row[\"avg_rating\"]),\n",
        "                          \"num_ratings\": row[\"num_ratings\"]\n",
        "                      } for row in avg_ratings}\n",
        "\n",
        "    print(f\"Created rating mapping for {len(rating_mapping)} books\")\n",
        "    return rating_mapping\n",
        "\n",
        "book_rating_mapping = create_book_rating_mapping(final_data)\n",
        "\n",
        "# Preview a few entries\n",
        "print(\"\\nSample book rating mappings:\")\n",
        "sample_books = list(book_rating_mapping.keys())[:5]\n",
        "for book_id in sample_books:\n",
        "    rating_info = book_rating_mapping[book_id]\n",
        "    print(f\"Book ID: {book_id} ---> Rating: {rating_info['avg_rating']:.2f}/5 ({rating_info['num_ratings']} reviews)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9HGwGLdME0C"
      },
      "source": [
        "We need genres for two thing:\n",
        "1. showing the genre of top books across all models.\n",
        "2. making topic sensitive pagerank\n",
        "<br>\n",
        "\n",
        "\n",
        "First we read it from the second csv file and we normalize its titles for comparing against ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EdaFsBAQME0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8dbddc-f4af-41bf-af7c-234f3423d0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded metadata for 165975 books with genres\n",
            "Created genre mapping for 100522 books\n",
            "Matched genres: 83214 (82.8%)\n",
            "\n",
            "Sample genre mappings:\n",
            "Book ID: 40576 ---> Genre: ['Family & Relationships']\n",
            "Book ID: 12034 ---> Genre: ['Gardening']\n",
            "Book ID: 59360 ---> Genre: ['Crafts & Hobbies']\n",
            "Book ID: 63534 ---> Genre: Unknown\n"
          ]
        }
      ],
      "source": [
        "# book genre - book id\n",
        "\n",
        "book_data = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"quote\", '\"') \\\n",
        "    .option(\"escape\", '\"') \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .csv(\"/content/books_data.csv\") \\\n",
        "    .select(\"Title\", \"categories\")\n",
        "\n",
        "book_data_norm = normalize_titles(book_data)\n",
        "\n",
        "genre_by_title = {row[\"Title_Norm\"]: row[\"categories\"]\n",
        "                  for row in book_data_norm.select(\"Title_Norm\", \"categories\").collect()\n",
        "                  if row[\"categories\"] is not None}\n",
        "\n",
        "print(f\"Loaded metadata for {len(genre_by_title)} books with genres\")\n",
        "\n",
        "def create_genre_mapping(df):\n",
        "    books = df.select(\"book_id\", \"Title_Norm\").distinct().collect()\n",
        "\n",
        "    genre_mapping = {row[\"book_id\"]: genre_by_title.get(row[\"Title_Norm\"], \"Unknown\")\n",
        "                    for row in books}\n",
        "\n",
        "    matched = sum(1 for genre in genre_mapping.values() if genre != \"Unknown\")\n",
        "    print(f\"Created genre mapping for {len(genre_mapping)} books\")\n",
        "    print(f\"Matched genres: {matched} ({matched/len(genre_mapping)*100:.1f}%)\")\n",
        "\n",
        "    return genre_mapping\n",
        "\n",
        "genre_mapping = create_genre_mapping(final_data)\n",
        "\n",
        "# Preview a few entries\n",
        "print(\"\\nSample genre mappings:\")\n",
        "for book_id in list(genre_mapping.keys())[:4]:\n",
        "    title = title_mapping.get(book_id, \"Unknown\")\n",
        "    genre = genre_mapping[book_id]\n",
        "    print(f\"Book ID: {book_id} ---> Genre: {genre}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb95umhdME0C"
      },
      "source": [
        "What genres are most common? You can use this info for gerring topic sensitive pagerank later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UUY5gEgjME0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97261cb2-8f0f-43eb-864f-7d8e1ba5fe3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 genres by book count:\n",
            "1. Unknown: 17308 books\n",
            "2. ['Fiction']: 13826 books\n",
            "3. ['Religion']: 4648 books\n",
            "4. ['History']: 4428 books\n",
            "5. ['Juvenile Fiction']: 3692 books\n",
            "6. ['Biography & Autobiography']: 3484 books\n",
            "7. ['Business & Economics']: 2866 books\n",
            "8. ['Computers']: 2440 books\n",
            "9. ['Social Science']: 1665 books\n",
            "10. ['Juvenile Nonfiction']: 1585 books\n"
          ]
        }
      ],
      "source": [
        "# Get the top 10 genres\n",
        "genre_counts = {}\n",
        "for genre in genre_mapping.values():\n",
        "    genre_counts[genre] = genre_counts.get(genre, 0) + 1\n",
        "\n",
        "sorted_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 genres by book count:\")\n",
        "for i, (genre, count) in enumerate(sorted_genres[:10]):\n",
        "    print(f\"{i+1}. {genre}: {count} books\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErT3L9DGME0C"
      },
      "source": [
        "# **PHASE TWO: BIULDING THE GRAPH**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24-t4hPX6PI"
      },
      "source": [
        "### What is the Graph logic?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDDZKWwUX3p"
      },
      "source": [
        "Writing a fucntion that makes the graph - Books connected if they share 2 or more reviews from at least 2 unique people. <br>\n",
        "BOOKA <--> BOOKB if USER-A and USER-B reviewed both A and B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm0nlxCFME0D"
      },
      "source": [
        "One more think to point out is that pagerank needs directed graph and since our book network has no direction, we have to double each link and make it **bidirectional**.\n",
        "<br>\n",
        "BOOK1 -> BOOK2\n",
        "BOOK1 <- BOOK2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DK-YeRt-ME0D"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "\n",
        "def build_book_graph(df, threshold, purpose):\n",
        "    user_books = df.select(\"user_id_int\", \"book_id\").cache()\n",
        "    book_pairs = user_books.alias(\"ub1\").join(\n",
        "        user_books.alias(\"ub2\"),\n",
        "        F.col(\"ub1.user_id_int\") == F.col(\"ub2.user_id_int\")\n",
        "    ).filter(\n",
        "        F.col(\"ub1.book_id\") < F.col(\"ub2.book_id\")\n",
        "    ).select(\n",
        "        F.col(\"ub1.book_id\").alias(\"src\"),\n",
        "        F.col(\"ub2.book_id\").alias(\"dst\")\n",
        "    )\n",
        "\n",
        "    edges_df = book_pairs.groupBy(\"src\", \"dst\").count() \\\n",
        "        .filter(F.col(\"count\") >= threshold) \\\n",
        "        .select(\"src\", \"dst\").cache()\n",
        "\n",
        "    edges_df = edges_df.union(edges_df.select(F.col(\"dst\").alias(\"src\"), F.col(\"src\").alias(\"dst\")))\n",
        "\n",
        "    vertices_df = edges_df.select(\"src\").union(edges_df.select(\"dst\")).distinct().withColumnRenamed(\"src\", \"id\")\n",
        "\n",
        "    graph = GraphFrame(vertices_df, edges_df)\n",
        "\n",
        "    if purpose == \"pagerank_builtin\":\n",
        "        # Only return GraphFrame for built-in PageRank\n",
        "        return {\"graph\": graph}\n",
        "    elif purpose == \"pagerank_rdd\":\n",
        "        # Return RDD of edges and the number of nodes for RDD-based algorithms\n",
        "        edges_rdd = graph.edges.rdd.map(lambda row: (row[\"src\"], row[\"dst\"]))\n",
        "        nodes_count = vertices_df.count()\n",
        "        return {\"edges_rdd\": edges_rdd, \"nodes_count\": nodes_count}\n",
        "    elif purpose == \"pagerank_python\":\n",
        "        # Return local lists for pure Python algorithms (be careful with large graphs!!!)\n",
        "        links = [(row[\"src\"], row[\"dst\"]) for row in edges_df.collect()]\n",
        "        pages = [row[\"id\"] for row in vertices_df.collect()]\n",
        "        return {\"pages\": pages, \"links\": links}\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown purpose: {purpose}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtxEwi-BME0D"
      },
      "source": [
        "# **PHASE THREE: OBTAINING A BENCHMRK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj0JPqsCME0D"
      },
      "source": [
        "Non personalized classic pagerak from the graphframe. Its results will be a testing ground for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On4hqFJ7ME0E"
      },
      "source": [
        "If you refer to the graphframe user giude you can see this: <br>\n",
        "link: https://graphframes.io/docs/_site/user-guide.html#pagerank\n",
        "\n",
        "You can see we have 2 oprions, either fixed iteration or fixed tolerance (not at the same time). I Went with fixed iterations, you can easily test the other one as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FZG3AJ2APifm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe44f64-3c70-4e8d-c2db-d1b47ba331f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data_1 = build_book_graph(final_data, threshold=2, purpose=\"pagerank_builtin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "L6t9w5-LME0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6b5735-5e07-45a0-91a5-a561037f3cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vertices in the graph: 7,283\n",
            "Number of edges in the graph: 106,028\n"
          ]
        }
      ],
      "source": [
        "# Count vertices and edges in the graph (just to know the size of the graph)\n",
        "num_vertices = data_1['graph'].vertices.count()\n",
        "num_edges = data_1['graph'].edges.count()\n",
        "\n",
        "print(f\"Number of vertices in the graph: {num_vertices:,}\")\n",
        "print(f\"Number of edges in the graph: {num_edges:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "q1Wh_OwFME0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ddf293-61c5-459e-c735-cbc20cbb1196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique connections (edges/2): 53,014\n"
          ]
        }
      ],
      "source": [
        "# since the graph was bidirectional, we can divide the number of edges by 2 to get the actual number of connections\n",
        "print(f\"Number of unique connections (edges/2): {num_edges // 2:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "edIWN_SWY1Ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce5e2f9-12cb-4c47-fdde-33f3d52ea3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        }
      ],
      "source": [
        "# running the built-in PageRank algorithm in oder to get a benchmark\n",
        "# ATTENTION: This may take some time and it is slow, run this at your own risk!\n",
        "\n",
        "result_one = data_1['graph'].pageRank(resetProbability=0.15, maxIter=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "HZdppNOCME0E"
      },
      "outputs": [],
      "source": [
        "# First extract the top 20 book IDs - the benchmark for PageRank\n",
        "# ATTENTION: This one too will take long, run this at your own risk!\n",
        "\n",
        "top_20_books = result_one.vertices.orderBy(result_one.vertices.pagerank.desc()).limit(20).collect()\n",
        "top_20_ids = [row[\"id\"] for row in top_20_books]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "pd0owybZME0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ccd6e2-8e14-4a0a-ffa9-f901e28a87b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Books by PageRank:\n",
            "--------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "--------------------\n",
            " 1   | pride and prejudice                      | ['Juvenile Nonfic... |    4.53    | 40.470887\n",
            " 2   | the great gatsby                         | ['Fiction']          |    4.18    | 32.696410\n",
            " 3   | wuthering heights                        | ['Fiction']          |    4.08    | 31.546651\n",
            " 4   | the catcher in the rye                   | ['Young Adult Fic... |    3.97    | 30.978957\n",
            " 5   | the hobbit                               | ['Juvenile Fiction'] |    4.67    | 30.261263\n",
            " 6   | to kill a mockingbird                    | ['Performing Arts']  |    4.60    | 29.508966\n",
            " 7   | fahrenheit 451                           | ['Comics & Graphi... |    4.20    | 27.744051\n",
            " 8   | of mice and men                          | ['Fiction']          |    4.37    | 26.925210\n",
            " 9   | brave new world                          | ['Reference']        |    4.25    | 26.654684\n",
            " 10  | the picture of dorian gray               | ['Fiction']          |    4.29    | 25.246845\n",
            " 11  | the scarlet letter a romance             | Unknown              |    3.90    | 24.854448\n",
            " 12  | jane eyre                                | ['Fiction']          |    4.56    | 24.592147\n",
            " 13  | great expectations                       | ['Juvenile Nonfic... |    4.19    | 23.572939\n",
            " 14  | emma                                     | ['Fiction']          |    4.24    | 21.226307\n",
            " 15  | ulysses                                  | ['Fiction']          |    3.77    | 20.446109\n",
            " 16  | atlas shrugged                           | ['Fiction']          |    4.03    | 20.022607\n",
            " 17  | lord of the flies                        | ['Fiction']          |    3.96    | 18.687344\n",
            " 18  | frankenstein                             | ['Frankenstein (F... |    4.09    | 18.144955\n",
            " 19  | east of eden                             | ['Fiction']          |    4.60    | 17.769116\n",
            " 20  | the sun also rises                       | ['Fiction']          |    3.98    | 17.304198\n",
            "\n",
            "Top 20 book IDs: [58130, 78919, 99734, 73915, 79741, 92606, 26028, 53131, 12373, 85093, 87124, 39483, 31577, 24210, 94339, 8286, 44600, 28578, 23226, 88699]\n"
          ]
        }
      ],
      "source": [
        "# Create a formatted table with all required information\n",
        "print(\"\\nTop 20 Books by PageRank:\")\n",
        "print(\"-\" * 20)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "for i, row in enumerate(top_20_books):\n",
        "    book_id = row[\"id\"]\n",
        "    pagerank_score = row[\"pagerank\"]\n",
        "\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "\n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "\n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "\n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")\n",
        "\n",
        "print(f\"\\nTop 20 book IDs: {top_20_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB3Seav7ME0F"
      },
      "source": [
        "The above list is our benchmark, ideally we want the result of our following models to be identical or at least very close to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDtWZvWdME0F"
      },
      "source": [
        "# **PHASE FOUR: ATTEMPTING TO WRITE THE ALGORITHM FROM SCRATCH**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY47zC-rME0F"
      },
      "source": [
        "First we write a function using only python, just to see if we can implement this logic ourselves or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IEFDci3pME0F"
      },
      "outputs": [],
      "source": [
        "def pagerank_python(pages, links, beta=0.85, max_iter=100, tol=1e-6, silent=False):\n",
        "\n",
        "    if not pages:\n",
        "        raise ValueError(\"Pages list cannot be empty\")\n",
        "    if not links:\n",
        "        raise ValueError(\"Links list cannot be empty\")\n",
        "    if not (0 <= beta <= 1):\n",
        "        raise ValueError(f\"Beta must be between 0 and 1, got {beta}\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(f\"Max iterations must be positive, got {max_iter}\")\n",
        "    if tol <= 0:\n",
        "        raise ValueError(f\"Tolerance must be positive, got {tol}\")\n",
        "\n",
        "    N = len(pages)\n",
        "    if not silent:\n",
        "        print(f\"Starting PageRank with {N:,} pages\")\n",
        "\n",
        "    ranks = {p: 1.0 / N for p in pages}\n",
        "    initial_sum = sum(ranks.values())\n",
        "    if not silent:\n",
        "        print(f\"Initialized ranks (sum={initial_sum:.8f})\")\n",
        "\n",
        "    adjacency = defaultdict(list)\n",
        "    for src, dst in links:\n",
        "        adjacency[src].append(dst)\n",
        "\n",
        "    nodes_with_outlinks = len(adjacency)\n",
        "    total_edges = len(links)\n",
        "    if not silent:\n",
        "        print(f\"Built adjacency list ({nodes_with_outlinks:,} nodes with outlinks, {total_edges:,} total edges)\")\n",
        "        print(f\"Starting PageRank iterations...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        new_ranks = {p: (1 - beta) / N for p in pages}\n",
        "\n",
        "        for src in pages:\n",
        "            neighbors = adjacency.get(src, [])\n",
        "            if not neighbors:\n",
        "                continue\n",
        "            share = ranks[src] / len(neighbors)\n",
        "            for dst in neighbors:\n",
        "                new_ranks[dst] += beta * share\n",
        "\n",
        "        diff = (sum((new_ranks[p] - ranks[p])**2 for p in pages))**0.5  # L2 norm\n",
        "        rank_sum = sum(new_ranks.values())\n",
        "\n",
        "        if not silent:\n",
        "            print(f\"Iteration {iteration + 1:3d}: L2 diff={diff:.6f}, sum={rank_sum:.8f}\")\n",
        "\n",
        "        ranks = new_ranks\n",
        "        if diff < tol:\n",
        "            break\n",
        "\n",
        "    if not silent:\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"Converged after {iteration + 1} iterations\")\n",
        "\n",
        "        final_sum = sum(ranks.values())\n",
        "        if abs(final_sum - 1.0) > 1e-6:\n",
        "            print(f\"Warning: Final sum ({final_sum:.8f}) deviates from 1.0\")\n",
        "        else:\n",
        "            print(f\"Rank sum validation passed\")\n",
        "\n",
        "    return sorted(ranks.items(), key=lambda x: -x[1]), iteration + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "U11tBT7VME0F"
      },
      "outputs": [],
      "source": [
        "# first we make the graph for this specific function\n",
        "data_2 = build_book_graph(final_data, threshold=2, purpose=\"pagerank_python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Jwdmhtl8ME0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ab579d-0aad-4b2d-fc5c-9c952a99efe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting PageRank with 7,283 pages\n",
            "Initialized ranks (sum=1.00000000)\n",
            "Built adjacency list (7,283 nodes with outlinks, 106,028 total edges)\n",
            "Starting PageRank iterations...\n",
            "------------------------------------------------------------\n",
            "Iteration   1: L2 diff=0.023783, sum=1.00000000\n",
            "Iteration   2: L2 diff=0.007672, sum=1.00000000\n",
            "Iteration   3: L2 diff=0.002466, sum=1.00000000\n",
            "Iteration   4: L2 diff=0.001287, sum=1.00000000\n",
            "Iteration   5: L2 diff=0.000847, sum=1.00000000\n",
            "Iteration   6: L2 diff=0.000657, sum=1.00000000\n",
            "Iteration   7: L2 diff=0.000505, sum=1.00000000\n",
            "Iteration   8: L2 diff=0.000420, sum=1.00000000\n",
            "Iteration   9: L2 diff=0.000341, sum=1.00000000\n",
            "Iteration  10: L2 diff=0.000287, sum=1.00000000\n",
            "Iteration  11: L2 diff=0.000239, sum=1.00000000\n",
            "Iteration  12: L2 diff=0.000203, sum=1.00000000\n",
            "Iteration  13: L2 diff=0.000170, sum=1.00000000\n",
            "Iteration  14: L2 diff=0.000144, sum=1.00000000\n",
            "Iteration  15: L2 diff=0.000122, sum=1.00000000\n",
            "Iteration  16: L2 diff=0.000104, sum=1.00000000\n",
            "Iteration  17: L2 diff=0.000088, sum=1.00000000\n",
            "Iteration  18: L2 diff=0.000075, sum=1.00000000\n",
            "Iteration  19: L2 diff=0.000063, sum=1.00000000\n",
            "Iteration  20: L2 diff=0.000054, sum=1.00000000\n",
            "Iteration  21: L2 diff=0.000046, sum=1.00000000\n",
            "Iteration  22: L2 diff=0.000039, sum=1.00000000\n",
            "Iteration  23: L2 diff=0.000033, sum=1.00000000\n",
            "Iteration  24: L2 diff=0.000028, sum=1.00000000\n",
            "Iteration  25: L2 diff=0.000024, sum=1.00000000\n",
            "Iteration  26: L2 diff=0.000020, sum=1.00000000\n",
            "Iteration  27: L2 diff=0.000017, sum=1.00000000\n",
            "Iteration  28: L2 diff=0.000015, sum=1.00000000\n",
            "Iteration  29: L2 diff=0.000012, sum=1.00000000\n",
            "Iteration  30: L2 diff=0.000011, sum=1.00000000\n",
            "Iteration  31: L2 diff=0.000009, sum=1.00000000\n",
            "Iteration  32: L2 diff=0.000008, sum=1.00000000\n",
            "Iteration  33: L2 diff=0.000006, sum=1.00000000\n",
            "Iteration  34: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  35: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  36: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  37: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  38: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  39: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  40: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  41: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  42: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  43: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  44: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  45: L2 diff=0.000001, sum=1.00000000\n",
            "------------------------------------------------------------\n",
            "Converged after 45 iterations\n",
            "Rank sum validation passed\n"
          ]
        }
      ],
      "source": [
        "result_two = pagerank_python(data_2['pages'], data_2['links'], beta=0.85, max_iter=100, tol=1e-6, silent=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "BAgwIop5ME0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "242e54cc-55eb-4c68-fe85-20564f6a858a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Books by Custom PageRank (converged in 45 iterations):\n",
            "------------------------------------------------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1   | pride and prejudice                      | ['Juvenile Nonfic... |    4.53    | 0.005558\n",
            " 2   | the great gatsby                         | ['Fiction']          |    4.18    | 0.004491\n",
            " 3   | wuthering heights                        | ['Fiction']          |    4.08    | 0.004333\n",
            " 4   | the catcher in the rye                   | ['Young Adult Fic... |    3.97    | 0.004255\n",
            " 5   | the hobbit                               | ['Juvenile Fiction'] |    4.67    | 0.004156\n",
            " 6   | to kill a mockingbird                    | ['Performing Arts']  |    4.60    | 0.004053\n",
            " 7   | fahrenheit 451                           | ['Comics & Graphi... |    4.20    | 0.003810\n",
            " 8   | of mice and men                          | ['Fiction']          |    4.37    | 0.003698\n",
            " 9   | brave new world                          | ['Reference']        |    4.25    | 0.003661\n",
            " 10  | the picture of dorian gray               | ['Fiction']          |    4.29    | 0.003468\n",
            " 11  | the scarlet letter a romance             | Unknown              |    3.90    | 0.003414\n",
            " 12  | jane eyre                                | ['Fiction']          |    4.56    | 0.003378\n",
            " 13  | great expectations                       | ['Juvenile Nonfic... |    4.19    | 0.003238\n",
            " 14  | emma                                     | ['Fiction']          |    4.24    | 0.002916\n",
            " 15  | ulysses                                  | ['Fiction']          |    3.77    | 0.002808\n",
            " 16  | atlas shrugged                           | ['Fiction']          |    4.03    | 0.002750\n",
            " 17  | lord of the flies                        | ['Fiction']          |    3.96    | 0.002567\n",
            " 18  | frankenstein                             | ['Frankenstein (F... |    4.09    | 0.002492\n",
            " 19  | east of eden                             | ['Fiction']          |    4.60    | 0.002441\n",
            " 20  | the sun also rises                       | ['Fiction']          |    3.98    | 0.002377\n",
            "\n",
            "Top 20 book IDs from custom PageRank: [58130, 78919, 99734, 73915, 79741, 92606, 26028, 53131, 12373, 85093, 87124, 39483, 31577, 24210, 94339, 8286, 44600, 28578, 23226, 88699]\n"
          ]
        }
      ],
      "source": [
        "# TOP 20 Books by Custom PageRank - PURE PYTHON implementation\n",
        "\n",
        "\n",
        "custom_pagerank_sorted, iterations_to_converge = result_two\n",
        "custom_pagerank_top20 = custom_pagerank_sorted[:20]\n",
        "custom_pagerank_ids = [book_id for book_id, score in custom_pagerank_top20]\n",
        "\n",
        "print(f\"\\nTop 20 Books by Custom PageRank (converged in {iterations_to_converge} iterations):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (book_id, pagerank_score) in enumerate(custom_pagerank_top20):\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "\n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "\n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "\n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")\n",
        "\n",
        "print(f\"\\nTop 20 book IDs from custom PageRank: {custom_pagerank_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jVf2gl8ME0G"
      },
      "source": [
        "# **PHASE FOUR (4.1): USING SPARK RDD FOR PAGERNAK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "F-9qF-krME0G"
      },
      "outputs": [],
      "source": [
        "data_3 = build_book_graph(final_data, threshold=2, purpose=\"pagerank_rdd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "aq_3FdVyME0G"
      },
      "outputs": [],
      "source": [
        "def pagerank_rdd(edges_rdd, nodes_count, damping_factor=0.85, max_iter=100, tolerance=1e-6, silent=False):\n",
        "\n",
        "\n",
        "    # Input validation\n",
        "    if not edges_rdd:\n",
        "        raise ValueError(\"Edges RDD cannot be empty\")\n",
        "    if nodes_count <= 0:\n",
        "        raise ValueError(f\"Nodes count must be positive, got {nodes_count}\")\n",
        "    if not (0 <= damping_factor <= 1):\n",
        "        raise ValueError(f\"Damping factor must be between 0 and 1, got {damping_factor}\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(f\"Max iterations must be positive, got {max_iter}\")\n",
        "    if tolerance <= 0:\n",
        "        raise ValueError(f\"Tolerance must be positive, got {tolerance}\")\n",
        "\n",
        "    if not silent:\n",
        "        print(f\"Starting PageRank RDD with {nodes_count:,} nodes\")\n",
        "\n",
        "    if not silent:\n",
        "        print(\"Building adjacency list...\")\n",
        "    adjacency_rdd = edges_rdd.groupByKey().mapValues(list).cache()\n",
        "    if not silent:\n",
        "        print(\"adjacency list built and cached\")\n",
        "\n",
        "    if not silent:\n",
        "        print(\"Identifying all nodes in the graph...\")\n",
        "    all_nodes = edges_rdd.flatMap(lambda x: [x[0], x[1]]).distinct().collect()\n",
        "\n",
        "    teleport_prob = (1 - damping_factor) / len(all_nodes)\n",
        "    if not silent:\n",
        "        print(f\"Teleportation probability set: {teleport_prob:.8f}\")\n",
        "\n",
        "    ranks = {node: 1.0 / len(all_nodes) for node in all_nodes}\n",
        "    initial_sum = sum(ranks.values())\n",
        "    if not silent:\n",
        "        print(f\"Initial ranks set (sum={initial_sum:.8f})\")\n",
        "        print(f\"Starting PageRank iterations...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        old_ranks = ranks.copy()\n",
        "\n",
        "        ranks_bc = edges_rdd.context.broadcast(ranks)\n",
        "\n",
        "        contributions = adjacency_rdd.flatMap(\n",
        "            lambda node_neighbors: [\n",
        "                (neighbor, damping_factor * ranks_bc.value[node_neighbors[0]] / len(node_neighbors[1]))\n",
        "                for neighbor in node_neighbors[1]\n",
        "            ]\n",
        "        ).reduceByKey(lambda a, b: a + b).collectAsMap()\n",
        "\n",
        "        ranks = {node: teleport_prob + contributions.get(node, 0) for node in all_nodes}\n",
        "\n",
        "        diff = sum((ranks[node] - old_ranks[node])**2 for node in all_nodes)**0.5 # L2 norm\n",
        "\n",
        "        total_sum = sum(ranks.values())\n",
        "\n",
        "        if not silent:\n",
        "            print(f\"Iteration {iteration + 1:3d}: L2 diff={diff:.6f}, sum={total_sum:.8f}\")\n",
        "\n",
        "        ranks_bc.unpersist()\n",
        "\n",
        "        if diff < tolerance:\n",
        "            if not silent:\n",
        "                print(\"-\" * 60)\n",
        "                print(f\"Converged after {iteration + 1} iterations\")\n",
        "            break\n",
        "\n",
        "    if not silent:\n",
        "        if iteration + 1 == max_iter:\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"Reached maximum iterations ({max_iter}) without full convergence\")\n",
        "\n",
        "        print(f\"Final rank sum: {sum(ranks.values()):.8f}\")\n",
        "\n",
        "        final_sum = sum(ranks.values())\n",
        "        if abs(final_sum - 1.0) > 1e-6:\n",
        "            print(f\"Warning: Final sum ({final_sum:.8f}) deviates from 1.0\")\n",
        "        else:\n",
        "            print(f\"Rank sum validation passed\")\n",
        "\n",
        "    adjacency_rdd.unpersist()\n",
        "\n",
        "    return sorted(ranks.items(), key=lambda x: -x[1]), (iteration + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3mZaNRiME0G"
      },
      "source": [
        "carefull, this takes longer than the python version to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "H9pmEQdPME0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742f4c1b-da6d-48e2-aa72-ce16c3c9dc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edges_rdd partitions: 400\n",
            "edges_rdd is cached??: False\n",
            "Starting PageRank RDD with 7,283 nodes\n",
            "Building adjacency list...\n",
            "adjacency list built and cached\n",
            "Identifying all nodes in the graph...\n",
            "Teleportation probability set: 0.00002060\n",
            "Initial ranks set (sum=1.00000000)\n",
            "Starting PageRank iterations...\n",
            "------------------------------------------------------------\n",
            "Iteration   1: L2 diff=0.023783, sum=1.00000000\n",
            "Iteration   2: L2 diff=0.007672, sum=1.00000000\n",
            "Iteration   3: L2 diff=0.002466, sum=1.00000000\n",
            "Iteration   4: L2 diff=0.001287, sum=1.00000000\n",
            "Iteration   5: L2 diff=0.000847, sum=1.00000000\n",
            "Iteration   6: L2 diff=0.000657, sum=1.00000000\n",
            "Iteration   7: L2 diff=0.000505, sum=1.00000000\n",
            "Iteration   8: L2 diff=0.000420, sum=1.00000000\n",
            "Iteration   9: L2 diff=0.000341, sum=1.00000000\n",
            "Iteration  10: L2 diff=0.000287, sum=1.00000000\n",
            "Iteration  11: L2 diff=0.000239, sum=1.00000000\n",
            "Iteration  12: L2 diff=0.000203, sum=1.00000000\n",
            "Iteration  13: L2 diff=0.000170, sum=1.00000000\n",
            "Iteration  14: L2 diff=0.000144, sum=1.00000000\n",
            "Iteration  15: L2 diff=0.000122, sum=1.00000000\n",
            "Iteration  16: L2 diff=0.000104, sum=1.00000000\n",
            "Iteration  17: L2 diff=0.000088, sum=1.00000000\n",
            "Iteration  18: L2 diff=0.000075, sum=1.00000000\n",
            "Iteration  19: L2 diff=0.000063, sum=1.00000000\n",
            "Iteration  20: L2 diff=0.000054, sum=1.00000000\n",
            "Iteration  21: L2 diff=0.000046, sum=1.00000000\n",
            "Iteration  22: L2 diff=0.000039, sum=1.00000000\n",
            "Iteration  23: L2 diff=0.000033, sum=1.00000000\n",
            "Iteration  24: L2 diff=0.000028, sum=1.00000000\n",
            "Iteration  25: L2 diff=0.000024, sum=1.00000000\n",
            "Iteration  26: L2 diff=0.000020, sum=1.00000000\n",
            "Iteration  27: L2 diff=0.000017, sum=1.00000000\n",
            "Iteration  28: L2 diff=0.000015, sum=1.00000000\n",
            "Iteration  29: L2 diff=0.000012, sum=1.00000000\n",
            "Iteration  30: L2 diff=0.000011, sum=1.00000000\n",
            "Iteration  31: L2 diff=0.000009, sum=1.00000000\n",
            "Iteration  32: L2 diff=0.000008, sum=1.00000000\n",
            "Iteration  33: L2 diff=0.000006, sum=1.00000000\n",
            "Iteration  34: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  35: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  36: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  37: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  38: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  39: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  40: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  41: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  42: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  43: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  44: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  45: L2 diff=0.000001, sum=1.00000000\n",
            "------------------------------------------------------------\n",
            "Converged after 45 iterations\n",
            "Final rank sum: 1.00000000\n",
            "Rank sum validation passed\n"
          ]
        }
      ],
      "source": [
        "# Extract the edges_rdd and nodes_count from your graph\n",
        "edges_rdd = data_3[\"edges_rdd\"]\n",
        "nodes_count = data_3[\"nodes_count\"]\n",
        "\n",
        "print(f\"edges_rdd partitions: {edges_rdd.getNumPartitions()}\")\n",
        "print(f\"edges_rdd is cached??: {edges_rdd.is_cached}\")\n",
        "\n",
        "edges_rdd_fixed = edges_rdd.repartition(4).cache() # in local you can change this to 8\n",
        "edges_rdd_fixed.count()\n",
        "\n",
        "ranks, iterations = pagerank_rdd(edges_rdd_fixed, nodes_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ch1eN2vUME0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093a37ec-06a4-4810-fa23-de645f2fdbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Books by RDD PageRank (converged in 45 iterations):\n",
            "------------------------------------------------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1   | pride and prejudice                      | ['Juvenile Nonfic... |    4.53    | 0.005558\n",
            " 2   | the great gatsby                         | ['Fiction']          |    4.18    | 0.004491\n",
            " 3   | wuthering heights                        | ['Fiction']          |    4.08    | 0.004333\n",
            " 4   | the catcher in the rye                   | ['Young Adult Fic... |    3.97    | 0.004255\n",
            " 5   | the hobbit                               | ['Juvenile Fiction'] |    4.67    | 0.004156\n",
            " 6   | to kill a mockingbird                    | ['Performing Arts']  |    4.60    | 0.004053\n",
            " 7   | fahrenheit 451                           | ['Comics & Graphi... |    4.20    | 0.003810\n",
            " 8   | of mice and men                          | ['Fiction']          |    4.37    | 0.003698\n",
            " 9   | brave new world                          | ['Reference']        |    4.25    | 0.003661\n",
            " 10  | the picture of dorian gray               | ['Fiction']          |    4.29    | 0.003468\n",
            " 11  | the scarlet letter a romance             | Unknown              |    3.90    | 0.003414\n",
            " 12  | jane eyre                                | ['Fiction']          |    4.56    | 0.003378\n",
            " 13  | great expectations                       | ['Juvenile Nonfic... |    4.19    | 0.003238\n",
            " 14  | emma                                     | ['Fiction']          |    4.24    | 0.002916\n",
            " 15  | ulysses                                  | ['Fiction']          |    3.77    | 0.002808\n",
            " 16  | atlas shrugged                           | ['Fiction']          |    4.03    | 0.002750\n",
            " 17  | lord of the flies                        | ['Fiction']          |    3.96    | 0.002567\n",
            " 18  | frankenstein                             | ['Frankenstein (F... |    4.09    | 0.002492\n",
            " 19  | east of eden                             | ['Fiction']          |    4.60    | 0.002441\n",
            " 20  | the sun also rises                       | ['Fiction']          |    3.98    | 0.002377\n",
            "\n",
            "Top 20 book IDs from RDD PageRank: [58130, 78919, 99734, 73915, 79741, 92606, 26028, 53131, 12373, 85093, 87124, 39483, 31577, 24210, 94339, 8286, 44600, 28578, 23226, 88699]\n"
          ]
        }
      ],
      "source": [
        "# TOP 20 Books by RDD PageRank implementation\n",
        "rdd_iterations = iterations\n",
        "rdd_pagerank_top20 = ranks[:20]\n",
        "rdd_pagerank_ids = [book_id for book_id, score in rdd_pagerank_top20]\n",
        "\n",
        "print(f\"\\nTop 20 Books by RDD PageRank (converged in {rdd_iterations} iterations):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (book_id, pagerank_score) in enumerate(rdd_pagerank_top20):\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "\n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "\n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "\n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")\n",
        "\n",
        "print(f\"\\nTop 20 book IDs from RDD PageRank: {rdd_pagerank_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQKN-Z8fME0H"
      },
      "source": [
        "## COMPARING THE RESULTS BETWEEN THREE LISTS <br>\n",
        "both in content (book titles) and their rankings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4eVp_8RxME0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae5606a-2c6e-4912-95fc-253a0c506966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common books in all three lists: 20/20\n",
            "Common books between Built-in and Python: 20/20\n",
            "Common books between Built-in and RDD: 20/20\n",
            "Common books between Python and RDD: 20/20\n",
            "\n",
            "Top 5 books comparison:\n",
            "Position 1: Built-in=58130, Python=58130, RDD=58130\n",
            "  Match: yes\n",
            "Position 2: Built-in=78919, Python=78919, RDD=78919\n",
            "  Match: yes\n",
            "Position 3: Built-in=99734, Python=99734, RDD=99734\n",
            "  Match: yes\n",
            "Position 4: Built-in=73915, Python=73915, RDD=73915\n",
            "  Match: yes\n",
            "Position 5: Built-in=79741, Python=79741, RDD=79741\n",
            "  Match: yes\n"
          ]
        }
      ],
      "source": [
        "# are they even similar?\n",
        "common_all = set(top_20_ids) & set(custom_pagerank_ids) & set(rdd_pagerank_ids)\n",
        "common_builtin_python = set(top_20_ids) & set(custom_pagerank_ids)\n",
        "common_builtin_rdd = set(top_20_ids) & set(rdd_pagerank_ids)\n",
        "common_python_rdd = set(custom_pagerank_ids) & set(rdd_pagerank_ids)\n",
        "\n",
        "# Print overlap statistics\n",
        "print(f\"Common books in all three lists: {len(common_all)}/20\")\n",
        "print(f\"Common books between Built-in and Python: {len(common_builtin_python)}/20\")\n",
        "print(f\"Common books between Built-in and RDD: {len(common_builtin_rdd)}/20\")\n",
        "print(f\"Common books between Python and RDD: {len(common_python_rdd)}/20\")\n",
        "\n",
        "# Check if the top books match across implementations\n",
        "print(\"\\nTop 5 books comparison:\")\n",
        "for i in range(5):\n",
        "    if i < len(top_20_ids) and i < len(custom_pagerank_ids) and i < len(rdd_pagerank_ids):\n",
        "        print(f\"Position {i+1}: Built-in={top_20_ids[i]}, Python={custom_pagerank_ids[i]}, RDD={rdd_pagerank_ids[i]}\")\n",
        "        match = (top_20_ids[i] == custom_pagerank_ids[i] == rdd_pagerank_ids[i])\n",
        "        print(f\"  Match: {'yes' if match else 'no'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYdbKSFrME0H"
      },
      "source": [
        "The only difference is 2,3 of positions. Neglegable since top 20 books are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfSTjspTME0I"
      },
      "source": [
        "# **PHASE FIVE: TOPIC SENSITIVE PAGERANK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3SGqs7MME0I"
      },
      "source": [
        "For this one too we will be using pure python and rdd version.\n",
        "The only diffrence is that here we dont teleport to ANY place, Instead we travel to destinations that we are intrested in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qHpmcGPRME0I"
      },
      "outputs": [],
      "source": [
        "def pagerank_python_topic_sensitive(pages, links, genre_mapping, target_genres=None,\n",
        "                                   beta=0.85, max_iter=100, tol=1e-6, silent=False):\n",
        "\n",
        "\n",
        "    # Input validation\n",
        "    if not pages:\n",
        "        raise ValueError(\"Pages list cannot be empty\")\n",
        "    if not links:\n",
        "        raise ValueError(\"Links list cannot be empty\")\n",
        "    if not isinstance(genre_mapping, dict):\n",
        "        raise ValueError(\"Genre mapping must be a dictionary\")\n",
        "    if not (0 <= beta <= 1):\n",
        "        raise ValueError(f\"Beta must be between 0 and 1, got {beta}\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(f\"Max iterations must be positive, got {max_iter}\")\n",
        "    if tol <= 0:\n",
        "        raise ValueError(f\"Tolerance must be positive, got {tol}\")\n",
        "\n",
        "    N = len(pages)\n",
        "    if not silent:\n",
        "        print(f\"Starting Topic-Sensitive PageRank with {N:,} pages\")\n",
        "\n",
        "    ranks = {p: 1.0 / N for p in pages}\n",
        "    initial_sum = sum(ranks.values())\n",
        "    if not silent:\n",
        "        print(f\"Initialized ranks (sum={initial_sum:.8f})\")\n",
        "\n",
        "    adjacency = defaultdict(list)\n",
        "    for src, dst in links:\n",
        "        adjacency[src].append(dst)\n",
        "\n",
        "    nodes_with_outlinks = len(adjacency)\n",
        "    total_edges = len(links)\n",
        "    if not silent:\n",
        "        print(f\"Built adjacency list ({nodes_with_outlinks:,} nodes with outlinks, {total_edges:,} total edges)\")\n",
        "\n",
        "    if target_genres is None:\n",
        "        # Original behavior: uniform teleportation to all pages\n",
        "        teleport_prob = (1 - beta) / N\n",
        "        teleport_probs = {p: teleport_prob for p in pages}\n",
        "        if not silent:\n",
        "            print(f\"Using uniform teleportation (original PageRank behavior)\")\n",
        "    else:\n",
        "        topic_pages = [p for p in pages if any(str(g) in str(genre_mapping.get(p, \"\")) for g in target_genres)]\n",
        "\n",
        "        if not topic_pages:\n",
        "            # going back to uniform if no target pages found\n",
        "            teleport_prob = (1 - beta) / N\n",
        "            teleport_probs = {p: teleport_prob for p in pages}\n",
        "            if not silent:\n",
        "                print(f\"Warning: No pages found for target genres {target_genres}. Using uniform teleportation.\")\n",
        "        else:\n",
        "            topic_teleport_prob = (1 - beta) / len(topic_pages)\n",
        "            teleport_probs = {p: topic_teleport_prob if p in topic_pages else 0.0 for p in pages}\n",
        "            if not silent:\n",
        "                print(f\"Topic-sensitive teleportation: {len(topic_pages):,} pages in {target_genres}\")\n",
        "\n",
        "    if not silent:\n",
        "        print(f\"Starting PageRank iterations...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        new_ranks = {p: teleport_probs[p] for p in pages}\n",
        "\n",
        "        for src in pages:\n",
        "            neighbors = adjacency.get(src, [])\n",
        "            share = ranks[src] / len(neighbors)\n",
        "            for dst in neighbors:\n",
        "                new_ranks[dst] += beta * share\n",
        "\n",
        "        diff = (sum((new_ranks[p] - ranks[p])**2 for p in pages))**0.5  # L2 norm\n",
        "        rank_sum = sum(new_ranks.values())\n",
        "\n",
        "        if not silent:\n",
        "            print(f\"Iteration {iteration + 1:3d}: L2 diff={diff:.6f}, sum={rank_sum:.8f}\")\n",
        "\n",
        "        ranks = new_ranks\n",
        "        if diff < tol:\n",
        "            break\n",
        "\n",
        "    if not silent:\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"Converged after {iteration + 1} iterations\")\n",
        "\n",
        "        final_sum = sum(ranks.values())\n",
        "        if abs(final_sum - 1.0) > 1e-6:\n",
        "            print(f\"Warning: Final sum ({final_sum:.8f}) deviates from 1.0\")\n",
        "        else:\n",
        "            print(f\"Rank sum validation passed\")\n",
        "\n",
        "    return sorted(ranks.items(), key=lambda x: -x[1]), iteration + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mtzLZPvZME0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39e1f10-ce0c-49b2-e3b0-9ae90792b32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Topic-Sensitive PageRank with 7,283 pages\n",
            "Initialized ranks (sum=1.00000000)\n",
            "Built adjacency list (7,283 nodes with outlinks, 106,028 total edges)\n",
            "Topic-sensitive teleportation: 3,102 pages in ['Fiction']\n",
            "Starting PageRank iterations...\n",
            "------------------------------------------------------------\n",
            "Iteration   1: L2 diff=0.023970, sum=1.00000000\n",
            "Iteration   2: L2 diff=0.008106, sum=1.00000000\n",
            "Iteration   3: L2 diff=0.002680, sum=1.00000000\n",
            "Iteration   4: L2 diff=0.001514, sum=1.00000000\n",
            "Iteration   5: L2 diff=0.001022, sum=1.00000000\n",
            "Iteration   6: L2 diff=0.000827, sum=1.00000000\n",
            "Iteration   7: L2 diff=0.000632, sum=1.00000000\n",
            "Iteration   8: L2 diff=0.000546, sum=1.00000000\n",
            "Iteration   9: L2 diff=0.000432, sum=1.00000000\n",
            "Iteration  10: L2 diff=0.000380, sum=1.00000000\n",
            "Iteration  11: L2 diff=0.000305, sum=1.00000000\n",
            "Iteration  12: L2 diff=0.000270, sum=1.00000000\n",
            "Iteration  13: L2 diff=0.000218, sum=1.00000000\n",
            "Iteration  14: L2 diff=0.000193, sum=1.00000000\n",
            "Iteration  15: L2 diff=0.000157, sum=1.00000000\n",
            "Iteration  16: L2 diff=0.000139, sum=1.00000000\n",
            "Iteration  17: L2 diff=0.000113, sum=1.00000000\n",
            "Iteration  18: L2 diff=0.000100, sum=1.00000000\n",
            "Iteration  19: L2 diff=0.000081, sum=1.00000000\n",
            "Iteration  20: L2 diff=0.000072, sum=1.00000000\n",
            "Iteration  21: L2 diff=0.000059, sum=1.00000000\n",
            "Iteration  22: L2 diff=0.000052, sum=1.00000000\n",
            "Iteration  23: L2 diff=0.000042, sum=1.00000000\n",
            "Iteration  24: L2 diff=0.000038, sum=1.00000000\n",
            "Iteration  25: L2 diff=0.000031, sum=1.00000000\n",
            "Iteration  26: L2 diff=0.000027, sum=1.00000000\n",
            "Iteration  27: L2 diff=0.000022, sum=1.00000000\n",
            "Iteration  28: L2 diff=0.000020, sum=1.00000000\n",
            "Iteration  29: L2 diff=0.000016, sum=1.00000000\n",
            "Iteration  30: L2 diff=0.000014, sum=1.00000000\n",
            "Iteration  31: L2 diff=0.000012, sum=1.00000000\n",
            "Iteration  32: L2 diff=0.000010, sum=1.00000000\n",
            "Iteration  33: L2 diff=0.000008, sum=1.00000000\n",
            "Iteration  34: L2 diff=0.000007, sum=1.00000000\n",
            "Iteration  35: L2 diff=0.000006, sum=1.00000000\n",
            "Iteration  36: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  37: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  38: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  39: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  40: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  41: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  42: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  43: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  44: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  45: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  46: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  47: L2 diff=0.000001, sum=1.00000000\n",
            "------------------------------------------------------------\n",
            "Converged after 47 iterations\n",
            "Rank sum validation passed\n"
          ]
        }
      ],
      "source": [
        "# lets choose a genre\n",
        "result_two_fiction = pagerank_python_topic_sensitive(data_2['pages'], data_2['links'],genre_mapping, target_genres=['Fiction'], beta=0.85, max_iter=100, tol=1e-6, silent=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "O6VR2IR2ME0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b7fcf1-7bc5-4c97-80c2-8dc6f5fa8bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Fiction Books (Topic-Sensitive PageRank, converged in 47 iterations):\n",
            "------------------------------------------------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1   | pride and prejudice                      | ['Juvenile Nonfic... |    4.53    | 0.006128\n",
            " 2   | the hobbit                               | ['Juvenile Fiction'] |    4.67    | 0.004701\n",
            " 3   | wuthering heights                        | ['Fiction']          |    4.08    | 0.004697\n",
            " 4   | the great gatsby                         | ['Fiction']          |    4.18    | 0.004563\n",
            " 5   | the catcher in the rye                   | ['Young Adult Fic... |    3.97    | 0.004469\n",
            " 6   | to kill a mockingbird                    | ['Performing Arts']  |    4.60    | 0.004309\n",
            " 7   | fahrenheit 451                           | ['Comics & Graphi... |    4.20    | 0.004082\n",
            " 8   | of mice and men                          | ['Fiction']          |    4.37    | 0.003882\n",
            " 9   | jane eyre                                | ['Fiction']          |    4.56    | 0.003722\n",
            " 10  | the picture of dorian gray               | ['Fiction']          |    4.29    | 0.003653\n",
            " 11  | the scarlet letter a romance             | Unknown              |    3.90    | 0.003516\n",
            " 12  | brave new world                          | ['Reference']        |    4.25    | 0.003407\n",
            " 13  | emma                                     | ['Fiction']          |    4.24    | 0.003262\n",
            " 14  | great expectations                       | ['Juvenile Nonfic... |    4.19    | 0.003146\n",
            " 15  | east of eden                             | ['Fiction']          |    4.60    | 0.002848\n",
            " 16  | lord of the flies                        | ['Fiction']          |    3.96    | 0.002720\n",
            " 17  | ulysses                                  | ['Fiction']          |    3.77    | 0.002709\n",
            " 18  | foundation                               | ['Education']        |    4.18    | 0.002674\n",
            " 19  | frankenstein                             | ['Frankenstein (F... |    4.09    | 0.002544\n",
            " 20  | the sun also rises                       | ['Fiction']          |    3.98    | 0.002457\n"
          ]
        }
      ],
      "source": [
        "# Extract the top 20 Fiction books from topic-sensitive PageRank\n",
        "fiction_pagerank_sorted, iterations = result_two_fiction\n",
        "fiction_pagerank_top20 = fiction_pagerank_sorted[:20]\n",
        "\n",
        "print(f\"\\nTop 20 Fiction Books (Topic-Sensitive PageRank, converged in {iterations} iterations):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (book_id, pagerank_score) in enumerate(fiction_pagerank_top20):\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "\n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "\n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "\n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "K1QyF3lsME0I"
      },
      "outputs": [],
      "source": [
        "def pagerank_rdd_topic_sensitive(edges_rdd, nodes_count, genre_mapping, target_genres=None,\n",
        "                                damping_factor=0.85, max_iter=100, tolerance=1e-6, silent=False):\n",
        "\n",
        "    # Input validation\n",
        "    if not edges_rdd:\n",
        "        raise ValueError(\"Edges RDD cannot be empty\")\n",
        "    if nodes_count <= 0:\n",
        "        raise ValueError(f\"Nodes count must be positive, got {nodes_count}\")\n",
        "    if not isinstance(genre_mapping, dict):\n",
        "        raise ValueError(\"Genre mapping must be a dictionary\")\n",
        "    if not (0 <= damping_factor <= 1):\n",
        "        raise ValueError(f\"Damping factor must be between 0 and 1, got {damping_factor}\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(f\"Max iterations must be positive, got {max_iter}\")\n",
        "    if tolerance <= 0:\n",
        "        raise ValueError(f\"Tolerance must be positive, got {tolerance}\")\n",
        "\n",
        "    if not silent:\n",
        "        print(f\"Starting Topic-Sensitive PageRank RDD with {nodes_count:,} nodes\")\n",
        "\n",
        "    if not silent:\n",
        "        print(\"Building adjacency list...\")\n",
        "    adjacency_rdd = edges_rdd.groupByKey().mapValues(list).cache()\n",
        "    if not silent:\n",
        "        print(\"Adjacency list built and cached\")\n",
        "\n",
        "    if not silent:\n",
        "        print(\"Identifying all nodes in the graph...\")\n",
        "    all_nodes = edges_rdd.flatMap(lambda x: [x[0], x[1]]).distinct().collect()\n",
        "\n",
        "    if target_genres is None:\n",
        "        teleport_prob = (1 - damping_factor) / len(all_nodes)\n",
        "        teleport_probs = {node: teleport_prob for node in all_nodes}\n",
        "        if not silent:\n",
        "            print(f\"Using uniform teleportation (original PageRank behavior)\")\n",
        "            print(f\"Teleportation probability set: {teleport_prob:.8f}\")\n",
        "    else:\n",
        "        topic_nodes = [n for n in all_nodes if any(str(g) in str(genre_mapping.get(n, \"\")) for g in target_genres)]\n",
        "\n",
        "        if not topic_nodes:\n",
        "\n",
        "            teleport_prob = (1 - damping_factor) / len(all_nodes)\n",
        "            teleport_probs = {node: teleport_prob for node in all_nodes}\n",
        "            if not silent:\n",
        "                print(f\"Warning: No nodes found for target genres {target_genres}. Using uniform teleportation.\")\n",
        "                print(f\"Teleportation probability set: {teleport_prob:.8f}\")\n",
        "        else:\n",
        "            topic_teleport_prob = (1 - damping_factor) / len(topic_nodes)\n",
        "            teleport_probs = {n: topic_teleport_prob if n in topic_nodes else 0.0 for n in all_nodes}\n",
        "            if not silent:\n",
        "                print(f\"Topic-sensitive teleportation: {len(topic_nodes):,} nodes in {target_genres}\")\n",
        "                print(f\"Topic teleportation probability: {topic_teleport_prob:.8f}\")\n",
        "\n",
        "    ranks = {node: 1.0 / len(all_nodes) for node in all_nodes}\n",
        "    initial_sum = sum(ranks.values())\n",
        "    if not silent:\n",
        "        print(f\"Initial ranks set (sum={initial_sum:.8f})\")\n",
        "        print(f\"Starting PageRank iterations...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        old_ranks = ranks.copy()\n",
        "\n",
        "        ranks_bc = edges_rdd.context.broadcast(ranks)\n",
        "\n",
        "        contributions = adjacency_rdd.flatMap(\n",
        "            lambda node_neighbors: [\n",
        "                (neighbor, damping_factor * ranks_bc.value[node_neighbors[0]] / len(node_neighbors[1]))\n",
        "                for neighbor in node_neighbors[1]\n",
        "            ]\n",
        "        ).reduceByKey(lambda a, b: a + b).collectAsMap()\n",
        "\n",
        "        ranks = {node: teleport_probs[node] + contributions.get(node, 0) for node in all_nodes}\n",
        "\n",
        "        diff = sum((ranks[node] - old_ranks[node])**2 for node in all_nodes)**0.5  # L2 norm\n",
        "        total_sum = sum(ranks.values())\n",
        "\n",
        "        if not silent:\n",
        "            print(f\"Iteration {iteration + 1:3d}: L2 diff={diff:.6f}, sum={total_sum:.8f}\")\n",
        "\n",
        "        ranks_bc.unpersist()\n",
        "\n",
        "        if diff < tolerance:\n",
        "            if not silent:\n",
        "                print(\"-\" * 60)\n",
        "                print(f\"Converged after {iteration + 1} iterations\")\n",
        "            break\n",
        "\n",
        "    if not silent:\n",
        "        if iteration + 1 == max_iter:\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"Reached maximum iterations ({max_iter}) without full convergence\")\n",
        "\n",
        "        print(f\"Final rank sum: {sum(ranks.values()):.8f}\")\n",
        "\n",
        "        final_sum = sum(ranks.values())\n",
        "        if abs(final_sum - 1.0) > 1e-6:\n",
        "            print(f\"Warning: Final sum ({final_sum:.8f}) deviates from 1.0\")\n",
        "        else:\n",
        "            print(f\"Rank sum validation passed\")\n",
        "\n",
        "    adjacency_rdd.unpersist()\n",
        "\n",
        "    return sorted(ranks.items(), key=lambda x: -x[1]), (iteration + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "yGld_WzzME0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f022511b-0e89-459f-ed52-4decb9246094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Topic-Sensitive PageRank RDD with 7,283 nodes\n",
            "Building adjacency list...\n",
            "Adjacency list built and cached\n",
            "Identifying all nodes in the graph...\n",
            "Topic-sensitive teleportation: 3,102 nodes in ['Fiction']\n",
            "Topic teleportation probability: 0.00004836\n",
            "Initial ranks set (sum=1.00000000)\n",
            "Starting PageRank iterations...\n",
            "------------------------------------------------------------\n",
            "Iteration   1: L2 diff=0.023970, sum=1.00000000\n",
            "Iteration   2: L2 diff=0.008106, sum=1.00000000\n",
            "Iteration   3: L2 diff=0.002680, sum=1.00000000\n",
            "Iteration   4: L2 diff=0.001514, sum=1.00000000\n",
            "Iteration   5: L2 diff=0.001022, sum=1.00000000\n",
            "Iteration   6: L2 diff=0.000827, sum=1.00000000\n",
            "Iteration   7: L2 diff=0.000632, sum=1.00000000\n",
            "Iteration   8: L2 diff=0.000546, sum=1.00000000\n",
            "Iteration   9: L2 diff=0.000432, sum=1.00000000\n",
            "Iteration  10: L2 diff=0.000380, sum=1.00000000\n",
            "Iteration  11: L2 diff=0.000305, sum=1.00000000\n",
            "Iteration  12: L2 diff=0.000270, sum=1.00000000\n",
            "Iteration  13: L2 diff=0.000218, sum=1.00000000\n",
            "Iteration  14: L2 diff=0.000193, sum=1.00000000\n",
            "Iteration  15: L2 diff=0.000157, sum=1.00000000\n",
            "Iteration  16: L2 diff=0.000139, sum=1.00000000\n",
            "Iteration  17: L2 diff=0.000113, sum=1.00000000\n",
            "Iteration  18: L2 diff=0.000100, sum=1.00000000\n",
            "Iteration  19: L2 diff=0.000081, sum=1.00000000\n",
            "Iteration  20: L2 diff=0.000072, sum=1.00000000\n",
            "Iteration  21: L2 diff=0.000059, sum=1.00000000\n",
            "Iteration  22: L2 diff=0.000052, sum=1.00000000\n",
            "Iteration  23: L2 diff=0.000042, sum=1.00000000\n",
            "Iteration  24: L2 diff=0.000038, sum=1.00000000\n",
            "Iteration  25: L2 diff=0.000031, sum=1.00000000\n",
            "Iteration  26: L2 diff=0.000027, sum=1.00000000\n",
            "Iteration  27: L2 diff=0.000022, sum=1.00000000\n",
            "Iteration  28: L2 diff=0.000020, sum=1.00000000\n",
            "Iteration  29: L2 diff=0.000016, sum=1.00000000\n",
            "Iteration  30: L2 diff=0.000014, sum=1.00000000\n",
            "Iteration  31: L2 diff=0.000012, sum=1.00000000\n",
            "Iteration  32: L2 diff=0.000010, sum=1.00000000\n",
            "Iteration  33: L2 diff=0.000008, sum=1.00000000\n",
            "Iteration  34: L2 diff=0.000007, sum=1.00000000\n",
            "Iteration  35: L2 diff=0.000006, sum=1.00000000\n",
            "Iteration  36: L2 diff=0.000005, sum=1.00000000\n",
            "Iteration  37: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  38: L2 diff=0.000004, sum=1.00000000\n",
            "Iteration  39: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  40: L2 diff=0.000003, sum=1.00000000\n",
            "Iteration  41: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  42: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  43: L2 diff=0.000002, sum=1.00000000\n",
            "Iteration  44: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  45: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  46: L2 diff=0.000001, sum=1.00000000\n",
            "Iteration  47: L2 diff=0.000001, sum=1.00000000\n",
            "------------------------------------------------------------\n",
            "Converged after 47 iterations\n",
            "Final rank sum: 1.00000000\n",
            "Rank sum validation passed\n"
          ]
        }
      ],
      "source": [
        "ranks_topic_sensitive, iterations = pagerank_rdd_topic_sensitive(edges_rdd_fixed, nodes_count, genre_mapping, target_genres=[\"Fiction\"], damping_factor=0.85, max_iter=100, tolerance=1e-6, silent=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BWUpY2plME0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0426405b-60c1-4332-addc-cc33de104111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Fiction Books (RDD Topic-Sensitive PageRank, converged in 47 iterations):\n",
            "------------------------------------------------------------\n",
            " #   |                  Title                   |        Genre         | Avg Rating |  PageRank \n",
            "----------------------------------------------------------------------------------------------------\n",
            " 1   | pride and prejudice                      | ['Juvenile Nonfic... |    4.53    | 0.006128\n",
            " 2   | the hobbit                               | ['Juvenile Fiction'] |    4.67    | 0.004701\n",
            " 3   | wuthering heights                        | ['Fiction']          |    4.08    | 0.004697\n",
            " 4   | the great gatsby                         | ['Fiction']          |    4.18    | 0.004563\n",
            " 5   | the catcher in the rye                   | ['Young Adult Fic... |    3.97    | 0.004469\n",
            " 6   | to kill a mockingbird                    | ['Performing Arts']  |    4.60    | 0.004309\n",
            " 7   | fahrenheit 451                           | ['Comics & Graphi... |    4.20    | 0.004082\n",
            " 8   | of mice and men                          | ['Fiction']          |    4.37    | 0.003882\n",
            " 9   | jane eyre                                | ['Fiction']          |    4.56    | 0.003722\n",
            " 10  | the picture of dorian gray               | ['Fiction']          |    4.29    | 0.003653\n",
            " 11  | the scarlet letter a romance             | Unknown              |    3.90    | 0.003516\n",
            " 12  | brave new world                          | ['Reference']        |    4.25    | 0.003407\n",
            " 13  | emma                                     | ['Fiction']          |    4.24    | 0.003262\n",
            " 14  | great expectations                       | ['Juvenile Nonfic... |    4.19    | 0.003146\n",
            " 15  | east of eden                             | ['Fiction']          |    4.60    | 0.002848\n",
            " 16  | lord of the flies                        | ['Fiction']          |    3.96    | 0.002720\n",
            " 17  | ulysses                                  | ['Fiction']          |    3.77    | 0.002709\n",
            " 18  | foundation                               | ['Education']        |    4.18    | 0.002674\n",
            " 19  | frankenstein                             | ['Frankenstein (F... |    4.09    | 0.002544\n",
            " 20  | the sun also rises                       | ['Fiction']          |    3.98    | 0.002457\n"
          ]
        }
      ],
      "source": [
        "# Extract top 20 Fiction books from the RDD-based topic-sensitive PageRank\n",
        "rdd_fiction_top20 = ranks_topic_sensitive[:20]\n",
        "\n",
        "print(f\"\\nTop 20 Fiction Books (RDD Topic-Sensitive PageRank, converged in {iterations} iterations):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'#':^4} | {'Title':^40} | {'Genre':^20} | {'Avg Rating':^10} | {'PageRank':^10}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (book_id, pagerank_score) in enumerate(rdd_fiction_top20):\n",
        "    title = title_mapping.get(book_id, \"Unknown Title\")\n",
        "    title_display = (title[:37] + \"...\") if len(title) > 40 else title\n",
        "\n",
        "    genre = genre_mapping.get(book_id, \"Unknown\")\n",
        "    genre_display = (genre[:17] + \"...\") if len(genre) > 20 else genre\n",
        "\n",
        "    rating_info = book_rating_mapping.get(book_id, {\"avg_rating\": 0.0})\n",
        "    avg_rating = rating_info[\"avg_rating\"]\n",
        "\n",
        "    print(f\"{i+1:^4} | {title_display:<40} | {genre_display:<20} | {avg_rating:^10.2f} | {pagerank_score:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_fAsgsuME0J"
      },
      "source": [
        "You can use the above functions for the classic pagerank too: <br>\n",
        "\n",
        "classic_ranks, iterations = pagerank_rdd_topic_sensitive(<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    edges_rdd_fixed, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    nodes_count, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    genre_mapping, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    target_genres=None,  # This makes it behave like classic PageRank<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    damping_factor=0.85, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    max_iter=100, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    tolerance=1e-6, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;    silent=False<br>\n",
        ")<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpajcb4-ME0J"
      },
      "source": [
        "You can use the above functions for the classic pagerank too: <br>\n",
        "\n",
        "classic_ranks, iterations = pagerank_python_topic_sensitive(<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;data_2['pages'], <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;data_2['links'],<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;genre_mapping, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;target_genres=None,  # This makes it behave like classic PageRank<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;beta=0.85, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;max_iter=100, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;tol=1e-6, <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;silent=False<br>\n",
        ")<br>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}